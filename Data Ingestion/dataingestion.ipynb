{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x198f6907eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader =TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x198ff3a3100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader('attention.pdf')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web-based loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(web_paths=(\"https://www.cricbuzz.com/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\",\n",
    "                                 )\n",
    "                     )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.cricbuzz.com/'}, page_content='')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '1996-08-27', 'Title': 'A Unified picture of glueball candidates $f_0(1500)$ and $f_0(1700)$', 'Authors': 'Marco Genovese', 'Summary': 'A simple mixing scheme describing the $f_0(1500)$ and the $f_0(1700)$ as\\nmixed states of a $\\\\bar s s$ meson and a digluonium is reconsidered at the\\nlight of new experimental data.'}, page_content='arXiv:hep-ph/9608451v1  27 Aug 1996\\nISN 96.93\\nAugust, 1996\\nA UNIFIED PICTURE OF GLUEBALL CANDIDATES f0(1500) AND f0(1700)\\nMarco Genovese ∗\\nInstitut des Sciences Nucl´eaires\\nUniversit´e Joseph Fourier–IN2P3-CNRS\\n53, avenue des Martyrs, F-38026 Grenoble Cedex, France\\nABSTRACT\\nA simple mixing scheme describing the f0(1500) and the f0(1700) as mixed states of a ¯ss meson\\nand a digluonium is reconsidered at the light of new experimental data.\\nIn QCD, because of self interaction of gluons, one expects the existence of particles composed of\\ngluons, which are called glueballs.\\nMany years have passed since the original prediction of the existence of these states [1], but, albeit\\nthe existence of many diﬀerent candidates, up to now none of the many resonances has been assigned\\nto a glueball.\\nRecently, two identiﬁcations of the lowest–lying scalar glueball have been proposed: one claim [2]\\nis that it corresponds to a resonance observed at 1500 MeV (which can be identiﬁed with the former\\nf0(1590), see [3]) that does not ﬁt the usual meson nonet; the second [4] identiﬁes this state with the\\nresonance observed at 1700 MeV, which was known as Θ(1700).\\nBoth the resonances have unusual decay properties for an ordinary ¯qq meson, appear in gluon–rich\\nchannels and are in the mass region where lattice QCD predicts the existence of a scalar glueball [4,5].\\nFurthermore, for the f0(1700), Ref. [4] claims that the branching ratios are in agreement with a lattice\\nQCD calculation and that the mass practically coincides with the predicted one.\\n∗Supported by EU Contract ERBFMBICT 950427\\nHowever, it must be emphasized that all available lattice calculations of the digluonium mass are\\nmade in the so–called quenched approximation, namely neglecting the creation of ¯qq pairs: this casts\\nsome doubts about the reliability of these predictions, which can be considered only as indicative.\\nAlso, the prediction of branching ratios in lattice QCD has been questioned [6].\\nAnyway, it is quite puzzling to observe in the region where the scalar digluonium state is predicted\\ntwo resonances not well ﬁtting the usual meson nonets and with unusual decay properties.\\nIn our opinion a common origin should be searched for both these states.\\nSome years ago, a\\nphenomenological scheme was proposed, consisting of a mixing between a glueball and a ¯ss state [7].\\nThe improved experimental data on these two resonances allow now quite a deeper investigation. In\\nthis paper this mixing scheme is reconsidered at the light of the new experimental inputs.\\nThe mixing scheme is\\n| f0(1500)⟩= cos α | gg⟩−sin α | ¯ss⟩\\n| f0(1700)⟩= sin α | gg⟩+ cos α | ¯ss⟩\\n(1)\\nGiven that the main f0(1700) decay is in the ¯KK channel, it is assumed that the | ¯qq⟩(¯uu and ¯dd)\\ncomponent is negligible [7].\\nOf course, generally speaking, one could choose any arbitrary mixing between the scalar digluonium,\\nthe light quark | ¯qq⟩and the | ¯ss⟩states. At the moment the experimental data do not yet allow to\\nexclude other choices, also because of the poor knowledge of the scalar nonet [8,9] compared to the\\nother meson multiplets (it is worth to remember that strong instantonic eﬀects are expected in the\\nscalar sector [10]).\\nFor example, quite a diﬀerent perspective is adopted in [2], based on a mixing of the 0++ glueball\\nwith a light–quark system giving two physical states corresponding to the f0(1500) and f0(1400). In\\n[11] the f0(1500) derives from a mixing of the | ¯qq⟩and the | ¯ss⟩states. Finally, in [12] an identiﬁcation\\nof f0(1500) with an unmixed glueball is suggested.\\nIn the following, besides the mixing hypothesis (1), it is also assumed that the f0(1500) decouples\\nfrom ¯KK, since no experimental evidence of this decay has been reported. This assumption permits\\nto relate the decay amplitudes for the gg and the ¯ss components through\\n⟨K+K−| f0(1500)⟩= 0 =\\n= cos α⟨K+K−| gg⟩−sin α⟨K+K−| ¯ss⟩\\n(2)\\nOf course this is quite an approximation, but it is the simplest one and permits to obtain a large\\npredictive power for the model. A small branching ratio of f0(1500) →K ¯K or some diﬀerent phase\\nbetween the two amplitudes should not spoil the essence of the model. The agreement among the\\nresults and the experimental data conﬁrms that this ansatz is acceptable.\\nTo obtain the mixing angle from\\n⟨K+K−| f0(1700)⟩= csc α⟨K+K−| gg⟩\\n(3)\\none uses the isospin relations\\nΓ(f →ππ) = 3/2 × Γ(f →π+π−)\\n(4)\\nΓ(f →KK) = 2 × Γ(f →K+K−);\\n(5)\\nrequests ⟨ππ | ¯ss⟩= 0 and assumes ﬂavour independence\\n⟨K+K−| gg⟩= ⟨π+π−| gg⟩.\\nUsing the experimental values [3]\\nB.R.[f0(1700) →ππ] = 0.039+0.002\\n−0.024\\nB.R.[f0(1700) →¯KK] = 0.38+0.09\\n−0.19\\n(6)\\nfrom Eq. (3), including the phase space factors p (meson momentum), one gets\\nsin(α) = 0.579+0.072\\n−0.095\\ncos(α) = 0.815+0.067\\n−0.051\\n(7)\\nFrom the mixing angle α, one can immediately estimate the glueball and the scalar ¯ss state masses\\n[7], using the relations:\\n \\n¯ss\\nǫ\\nǫ\\ngg\\n!  \\n−sin α\\ncos α\\n!\\n= 1.503\\n \\n−sin α\\ncos α\\n!\\n(8)\\n \\n¯ss\\nǫ\\nǫ\\ngg\\n!  \\ncos α\\nsin α\\n!\\n= 1.697\\n \\ncos α\\nsin α\\n!\\n(9)\\nwhere gg denotes the digluonium mass, ¯ss the quark state mass and ǫ the mixing parameter.\\nThis gives both ¯ss and gg masses around 1.6 GeV, the latter in agrement with lattice predictions\\n[5,4].\\nThe mixing angle gives also access to many branching ratios. Using Ref. [2] and Eq. (3) one gets\\nthe following amplitudes relative to ⟨ππ | gg⟩:\\n⟨ππ | ¯ss⟩= 0\\n⟨K ¯K | ¯ss⟩= R cot(α)\\n⟨ηη | ¯ss⟩= 2R sin2(φ) cot(α)\\n⟨ηη′ | ¯ss⟩= −2R cos(φ) sin(φ) cot(α)\\n⟨K ¯K | gg⟩= R\\n⟨ηη | gg⟩= cos2(φ) + R2 sin2(φ)\\n⟨ηη′ | gg⟩= cos(φ) sin(φ)(1 −R2) ,\\n(10)\\nwhere R = ⟨¯ss | gg⟩/⟨¯qq | gg⟩measurs the breaking of SUf(3) in gluonium decays (u and d quarks\\nare assumed to be equivalent). No ﬂavour violation is considered for the decay of quarkonium in pair\\nof mesons (empirically this violation is shown to be quite small for the well–established meson nonets\\n[13]).\\nIn Eq. (10), φ is the angle for the η – η′ mixing\\nη = cos(φ)|¯qq⟩−sin(φ)|¯ss⟩\\nη′ = sin(φ)|¯qq⟩+ cos(φ)|¯ss⟩\\n(11)\\nIn the following, a value φ = 72o [14] is adopted; a possible gluonic component of the η and η′ (see\\nfor example [15] and references therein) will not be considered here.\\nFurthermore, considering that glueballs should, at least in a ﬁrst approximation, exhibit ﬂavour\\ndemocracy, ﬂavour–independence for the gg decays (i.e. R = 1) as well is assumed.\\nSome predictions are listed here and compared with the data of Ref. [3]. The calculations include\\na sum over permutation and over the various charge combinations with the appropriate weighting\\nfactors (4 for K ¯K, 3 for ππ, 2 for ηη′ and 1 for ηη). The theoretical errors only account for the\\nuncertainty on the mixing angle.\\nΓ(f0(1700) →ηη)\\nΓ(f0(1700) →ππ) = pη\\n3pπ\\n·\\n\\x02\\n1 + 2 sin2(φ) cot2(α)\\n\\x032 = 5.43+3.0\\n−2.4\\n(12)\\nin agreement with the experimental datum 4.6+2.9\\n−3.3;\\nΓ(f0(1700) →ηη)\\nΓ(f0(1700) →K ¯K) = pη\\n4pK\\n·\\n\\x02\\nsin2(α) + 2 sin2(φ) cos2(α)\\n\\x032 = 0.558+0.080\\n−0.061\\n(13)\\nin good agreement with the datum 0.47+0.24\\n−0.35.\\nSimilarly one ﬁnds:\\nΓ(f0(1500) →π0π0)\\nΓ(f0(1500) →ηη)\\n= pπ\\npη\\n\\x14\\n1\\ncos2(φ) −sin2(φ)\\n\\x152\\n= 2.2\\n(14)\\nin fair agreement with the two experimental data 1.45 ± 0.61, 2.12 ± 0.81.\\nA little more complicate is the evaluation of Γ(f0(1500) →ηη′)/Γ(f0(1500) →ηη) because f0(1500)\\njust lies at the threshold for ηη′ production. One has to consider a weighting with Breit-Wigner\\ndistribution through:\\nΓ(f0(1500) →ηη′)\\nΓ(f0(1500) →ηη) =\\n2 ·\\n\\x14 2 cos(φ) sin(φ)\\ncos2(φ) −sin2(φ)\\n\\x152\\n·\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nR ∞\\nmη+mη′ dE/[(2(E−Mf0)/Γ)2+1]·\\nq\\n(E2−m2η−m2\\nη′ )2−4m2ηm2\\nη′\\n4E4\\npη\\nMf0\\nR ∞\\n0\\ndE/[(2(E−Mf0 )/Γ)2+1]\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n= 0.232\\n(15)\\nto be compared with the value 0.29 ± 0.10, which is the Crystal Barrel result reported in Ref. [3].\\nAnalogously one has:\\nΓ(f0(1700) →ηη′)\\nΓ(f0(1700) →K ¯K) =\\n1/2 ·\\n\\x02\\n2 cos(φ) sin(φ) cos2(α)\\n\\x032 ·\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nR ∞\\nmη+mη′ dE/[(2(E−Mf0 )/Γ)2+1]·\\nq\\n(E2−m2η−m2\\nη′ )2−4m2ηm2\\nη′\\n4E4\\npK\\nMf0\\nR ∞\\n0\\ndE/[(2(E−Mf0 )/Γ)2+1]\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n= 0.039+0.013\\n−0.010 ,\\n(16)\\nto be compared with future experimental data.\\nFurther predictions may be obtained considering the decay of J/Ψ (Υ) to f0(1500) and f0(1700) to\\nproceed mainly through the gluonic component of these states. This assumption leads to:\\nΓ(J/Ψ →γf0(1700))\\nΓ(J/Ψ →γf0(1500) = tan2(α) ·\\n\"\\nM 2\\nJ/Ψ −1.6972\\nM 2\\nJ/Ψ −1.5032\\n#3\\n= 0.39+0.11\\n−0.14,\\n(17)\\nΓ(Υ →γf0(1700))\\nΓ(Υ →γf0(1500) = 0.50+0.14\\n−0.18,\\n(18)\\nAt the moment only the branching ratios B.R.[J/Ψ →γf0(1700) →γK ¯K)] = (9.7 ± 1.2) · 10−4\\nand B.R.[J/Ψ →γf0(1500) →γ4π)] = (8.2 ± 1.5) · 10−4 are available for the J/Ψ decays and only an\\nupper limit for f0(1700) in the Υ case. Thus, due to the missing of the knowledge of Γ(f0(1500) →\\n4π)/Γ(total), no real conclusion can still be drawn. When more stringent experimental results will be\\navailable the ratios (17,18) will represent a further test of this model.\\nFinally, another test of the model can be made considering the two–photon decays. Because gluons\\ndecouple from photons, this decay can proceed only through the quark component and is therefore\\npartially suppressed for f0(1500) and f0(1700). In the mixing scheme (1), one expects\\nΓ(f0(1700) →γγ)\\nΓ(f0(1500) →γγ) =\\n\\x121.697\\n1.503\\n\\x133\\n· cot2(α) = 2.85+1.0\\n−0.8\\n(19)\\nIf the f0(1400) is assumed to be the light quark isoscalar member of the 0++ nonet (but this\\nassignation is still quite controversial [3,8]) one also predicts\\nΓ(f0(1700) →γγ)\\nΓ(f0(1400) →γγ) = 0.095+0.016\\n−0.012\\n(20)\\nand\\nΓ(f0(1500) →γγ)\\nΓ(f0(1400) →γγ) = 0.033+0.008\\n−0.011\\n(21)\\nIn summary, it has been reconsidered a very simple mixing scheme which enables us to reproduce\\nall the available experimental data (albeit still not very rich) on the two glueball candidates f0(1500)\\nand f0(1700). In this scheme both resonances have a gluonic component mixed with a ¯ss one.\\nThe mixing angle has been obtained by a simple ansatz, allowing the evaluation of several ratios of\\nbranching ratios of the two resonances, in good agreement with the available experimental data.\\nUsing the predictions reported in this paper it will be possible to test with a larger accuracy the\\nmodel in a next future, when new experimental results will appear, permitting a clearer understanding\\nof the nature of these two peculiar particles and hopefully a ﬁrst certain identiﬁcation of a glueball.\\nClarifying the nature of these two resonances will be of great help for understanding of the compo-\\nsition of the 0++ meson nonet, which is still quite controversial [8,9].\\nAcknowledgement\\nThanks are due to M. Anselmino and J.-M. Richard for useful comments.\\n[1] H.Fritzsch, M.Gell-Mann Proc. of the XVI Int. Conf. on High Energy Phys. Vol.2 135 Batavia (1972);\\n[2] C.Amsler and F.E. Close, Phys. Let. B 353, 385 (1996), Phys. Rev. D 53, 295 (1996);\\n[3] Review of Particle Properties, R. M. Barnet et al., Phys Rev. D 54 (1996);\\n[4] D. Weingarten, Nucl. Phys. B (Proc. Suppl.) 34, 29 (1994);\\n[5] G. Bali et al (UKQCD), Phys. Lett B 309 378 (1993);\\n[6] G. Martinelli, plenary talk at XIV Particles and Nuclei International Conference, Williamsburg (USA)\\nMay 1996;\\n[7] M. Genovese, Phys. Rev D 46, 5204 (1992);\\n[8] M.R. Pennington, DTP 95/86 HADRON 95 summary;\\n[9] M. Genovese, Nuovo Cim. A 107 N.8, 1249 (1994) and references therein;\\n[10] E. V. Shuryak, Nuclear Phys. B214, 237 (1983); W. H. Blask et al., Z. Phys. A 337, 327 (1990); A. I.\\nVainshtein, V. I. Zakharov, V. A. Novikov, and M. A. Shifman, Sov. J. Part. Nucl. 13, 224 (1982);\\n[11] E. Klempt et al, Phys. Lett. B 361, 160 (1995);\\n[12] V. V. Anisovich, Phys. Lett. B 364, 195 (1996);\\n[13] S. Godfrey and N. Isgur, Phys. Rev. D 32, 189 (1985);\\n[14] C. Amsler et al. (Crystal Barrel Coll.), Phys. Lett. B 294, 451 (1992);\\n[15] M. Genovese, D. B. Lichtenberg and E. Predazzi, Zeit. f¨ur Phys. C 61, 425 (1994);\\n'),\n",
       " Document(metadata={'Published': '2017-02-23', 'Title': 'Role of a triangle singularity in the $πΔ$ decay of the $N(1700)(3/2^-)$', 'Authors': 'L. Roca, E. Oset', 'Summary': 'We show the important role played by the $\\\\pi\\\\Delta(1232)$ channel in the\\nbuild up of the $N(1700)(3/2^-)$ resonance due to the non-trivial enhancement\\nproduced by a singularity of a triangular loop. The $N(1700)$ is one of the\\ndynamically generated resonances produced by the coupled channel vector-baryon\\ninteraction. The $\\\\pi\\\\Delta$ channel was neglected in previous works but we\\nshow that it has to be incorporated into the coupled channel formalism due to\\nan enhancement produced by a singularity in the triangular loop with $\\\\rho$,\\nnucleon and $\\\\pi$ as internal loop lines and $\\\\pi$ and $\\\\Delta$ as external\\nones. The enhancement is of non-resonant origin but it contributes to the\\ndynamical generation of the $N(1700)$ resonance due to the non-linear dynamics\\ninvolved in the coupled channel mechanisms. We obtain an important increase of\\nthe total width of the $N(1700)$ resonance when the $\\\\pi\\\\Delta$ channel is\\nincluded and provide predictions for the partial widths of the $N(1700)$ decays\\ninto $VB$ and $\\\\pi\\\\Delta$.'}, page_content='arXiv:1702.07220v1  [hep-ph]  23 Feb 2017\\nRole of a triangle singularity in the π∆decay of the N(1700)(3/2−)\\nL. Roca\\nDepartamento de F´ısica, Universidad de Murcia, E-30100 Murcia, Spain\\nE. Oset\\nDepartamento de F´ısica Te´orica and IFIC, Centro Mixto Universidad de Valencia-CSIC\\nInstitutos de Investigaci´on de Paterna, Aptdo. 22085, 46071 Valencia, Spain\\n(Dated: July 21, 2018)\\nWe show the important role played by the π∆(1232) channel in the build up of the N(1700)(3/2−)\\nresonance due to the non-trivial enhancement produced by a singularity of a triangular loop. The\\nN(1700) is one of the dynamically generated resonances produced by the coupled channel vector-\\nbaryon interaction. The π∆channel was neglected in previous works but we show that it has to be\\nincorporated into the coupled channel formalism due to an enhancement produced by a singularity\\nin the triangular loop with ρ, nucleon and π as internal loop lines and π and ∆as external ones.\\nThe enhancement is of non-resonant origin but it contributes to the dynamical generation of the\\nN(1700) resonance due to the non-linear dynamics involved in the coupled channel mechanisms. We\\nobtain an important increase of the total width of the N(1700) resonance when the π∆channel is\\nincluded and provide predictions for the partial widths of the N(1700) decays into V B and π∆.\\nPACS numbers:\\nINTRODUCTION\\nThe N(1700)(3/2−) is catalogued in the PDG [1] as\\na three star resonance, in spite of which there is a large\\ndispersion of the results for its properties from diﬀerent\\ngroups as can be seen in Table I. As one can see, these\\nRef. Mass [MeV] Width [MeV]\\nΓπ∆/Γ\\nS-wave\\nΓπ∆/Γ\\nD-wave\\nΓρN/Γ\\nS-wave\\n[2]\\n1800 ± 35\\n400 ± 100\\n50-80%\\n4-14%\\n[3]\\n1675 ± 25\\n90 ± 40\\n[4]\\n1731 ± 15\\n110 ± 30\\n[5]\\n1790 ± 40\\n390 ± 140\\n72 ± 23%\\n< 10%\\n[6]\\n1665 ± 3\\n56 ± 8\\n31 ± 9%\\n3 ± 2%\\n38 ± 6%\\n[7]\\n1817 ± 22\\n134 ± 37\\n[8]\\n1736 ± 33\\n175 ± 133\\n11 ± 1% 79 ± 56% 7 ± 1%\\nTABLE I: Mass and width of the N(1700) at the PDG[1].\\nresults are quite diﬀerent, some times incompatible, and\\none might be even tempted to think that there could\\nbe two states, one around 1700 MeV and the other one\\naround 1800 MeV. The PDG values are M=1650-1750\\nMeV (1700 average) and Γ = 100-250 MeV (150 aver-\\nage). It also quotes values for the branching ratios chosen\\nfrom some experiments and analyses. The values quoted\\nfor the π∆branching ratio are those from [2] shown in\\nTable I, and the the ρN appears as ”seen”.\\nTheoretically there is a nice interpretation for a JP =\\n3/2−state around 1700 MeV from the study of the\\nvector-octet baryon interaction in [9]. Indeed, by using\\nthe coupled channels ρN, ωN, φN, K∗Λ, K∗Σ, and the\\ndriving force from the local hidden gauge approach [10–\\n12], plus the coupled Bethe-Salpeter equations, which im-\\npose unitarity, the vector-baryon (VB) scattering matrix\\nis obtained. Inspection of the poles shows two structures,\\none around 1700 MeV, which couples mostly to ρN, and\\nanother one around 1980 MeV, which couples mostly to\\nK∗Σ. The states appear in S-wave and correspond to\\nJP = 3/2−states. Although the state obtained at 1700\\nMeV corresponds formally to a ρN bound state for the\\nnominal mass of the ρ, the consideration of the mass\\ndistribution of the ρ in [9] allows the state obtained to\\ndecay into ρN and the state shows up with a width of\\naround 100 MeV, a bit short of the 150 MeV for the av-\\nerage of the PDG. The other shortcoming of the model\\nis that it does not contain the π∆channel which ex-\\nperimentally accounts for a large fraction of the width.\\nOne should, however, take into account that it is quite\\ncommon in dynamically generated resonances that some\\nchannels relevant in the decay play a smaller role in the\\nstructure of the state.\\nIndeed, one can have a bound\\nstate of some component (ρN in the present case) which\\ndecays to some open channel, and the strength of this\\ndecay depends much on the phase space available.\\nIn\\nthe present case, from 1700 MeV there is plenty of phase\\nspace for π∆decay.\\nIn the present work we show that the relevance of the\\nπ∆channel is tied to a peculiar mechanism involving a\\ntriangle mechanism. The issue of triangle singularities\\nwas introduced by Landau [13] and is catching renewed\\ninterest nowadays when a vast amount of empirical infor-\\nmation in hadron physics is available, and new examples\\nof triangle singularities are showing up [14]. A triangle\\ndiagram appears in the case of a particle A decaying into\\n1+2, particle 2 decaying to B+3 and particles 1+3 merg-\\ning into another particle C. This kind of triangle diagram\\ndoes not always produce a singularity. It requires that\\n2\\nthe particles are collinear and that the process can occur\\nat the classical level, Coleman-Norton theorem [15]. The\\nﬁeld theoretical amplitude becomes inﬁnite if the inter-\\nmediate particles are stable. In real cases, some of these\\nparticles have a ﬁnite width and the inﬁnity gives rise\\nto a ﬁnite peak, which usually has important experimen-\\ntal consequences. Sometimes the peak resembles pretty\\nmuch a resonance and can be misinterpreted as such.\\nTechnically, the amplitudes involving a loop integral\\nare usually solved using the Feynman parametrization\\nand dispersion relations. An easier and more intuitive\\nformulation has been given recently in Ref. [16], per-\\nforming analytically the energy integration and looking\\nexplicitly to the poles in the remaining integral. A sim-\\nple equation is obtained to see where a singularity will\\nappear, and the ﬁnal integral is also easy to perform.\\nOne recent example of a process where the trian-\\ngle singularities is relevant is the η(1405) →π a0(980)\\nand η(1405) →π f0(980) [17–19].\\nIn particular the\\nη(1405) →π f0(980) process is isospin forbidden and it\\nbecomes largely enhanced due to a triangle singularity\\nwhich involves η(1405) →K∗¯K followed by K∗→K π\\nand the merging of K ¯K to give the f0(980). Another\\nexample is the case of the “a1(1420)” originally advo-\\ncated by the COMPASS collaboration as a new reso-\\nnance [20].\\nYet, as suggested in Ref. [14] and proved\\nin Refs. [21, 22], the peak observed at COMPASS comes\\nfrom the πf0(980) decay of the a1(1260), via a triangle\\nsingularity proceeding through a1 →K∗¯K, K∗→Kπ\\nand K ¯K →f0(980). Not surprisingly, the f1(1420) cata-\\nlogued in the PDG as a resonance, found a similar inter-\\npretation as the decay mode of the f1(1285) into πa0(980)\\nand K∗¯K [23]. Sometimes a triangle singularity has a\\ndirect inﬂuence in a reaction even if no resonance has\\nbeen associated to the peak that it creates. This is the\\ncase in the γp →K+Λ(1405) reaction [24]. The process\\nγp →K∗Σ, K∗→Kπ, Σπ →Λ(1405) leads to a peak\\nin the cross section around √s = 2120 MeV that conven-\\ntional approaches failed to describe. Another example\\nin this line is the role of the triangle singularity in the\\nπN(1535) contribution to the γp →π0ηp reaction close\\nto threshold [25], which in [26] is described in terms of a\\ntriangle singularity involving the ∆(1700) →η∆followed\\nby ∆→πN and ηN →N(1535).\\nSimilarly, the f2(1810) is also explained as a conse-\\nquence of the f2(1650) →K∗¯K∗, K∗→πK and K ¯K∗\\nmerging into the a1(1260) [27]. More examples can be\\nfound in Refs. [28–32].\\nIn some cases the singularity tied to the decay of a\\nresonance does not lead to a new peak at a diﬀerent en-\\nergy, but it comes precisely at the same energy where the\\nresonance appears and reinforces it introducing new de-\\ncay channels. This is the case of the N(1875) resonance\\nstudied in [33], which develops a singularity at the same\\nenergy from the resonance coupling to Σ∗K, Σ∗→πΛ\\nand KΛ →N(1535). In this case the singularity rein-\\nforces the resonance adding to it a new relevant channel.\\nThere is no way a priori to know if a triangle singular-\\nity can be relevant or not in a given process. One must\\nperform the detailed evaluation and see what comes out.\\nIn this direction it is worth commenting on the sugges-\\ntion that the narrow peak of the J/ψ p invariant mass at\\n4450 MeV seen by the LHCb collaboration [34, 35] could\\nbe due to a triangle singularity with Λb →Λ(1890)χc1,\\nΛ(1890) →\\n¯Kp, pχc1 →J/ψp [36, 37].\\nHowever, as\\nshown in [16], assuming the preferred experimental quan-\\ntum numbers of this peak, 3/2−, 5/2+, the χc1p →J/ψp\\nproceeds with χc1p in p-wave or d-wave and the χc1p is\\nat threshold at exactly 4450 MeV, hence, this amplitude\\nvanishes there on shell and the suggested process cannot\\nbe responsible for the observed peak.\\nIn the present work we show another example of a tri-\\nangle singularity appearing at the energy where a reso-\\nnance already exists. Indeed, the N(1700)(3/2−) is well\\naccepted and, as mentioned above, can be interpreted as\\nmostly a ρN bound state. The singularity appears from\\nthe mechanism of N(1700) →ρN followed by ρ →ππ\\nand fusion of πN to give the ∆(1232). The singularity\\nof this process appears around 1700 MeV and introduces\\nthe π∆channel as an important building block into the\\nwave function of the resonance.\\nFORMALISM\\nThe vector-baryon channels in the N(1700)\\nThe N(1700) resonance, (I = 1/2, JP = 3/2−), was\\none among several resonances obtained dynamically in\\nref. [9] from the interaction of the lowest mass vector-\\nmeson (V ) octet and the lowest mass baryon (B) octet\\nin coupled channels. In particular, the N(1700) arises\\ndynamically from the ρN, ωN, φN, K∗Λ and K∗Σ in-\\nteraction with isospin I = 1/2 and in S-wave. The domi-\\nnant channel is ρN and also, but less important, K∗Λ. It\\nis very important to stress that, using the techniques of\\nthe chiral unitary approach, the N(1700) arises naturally\\nfrom the coupled channel dynamics without the need to\\ninclude the N(1700) as an explicit degree of freedom. We\\nwill start from the model of ref. [9] and, in the next sec-\\ntion, we will improve upon it by adding the π∆channel,\\nwhich produces the triangular singularity that will con-\\ntribute signiﬁcantly to the dynamics that generates the\\nN(1700).\\nTherefore, for the sake of completeness, we\\nbrieﬂy summarize the approach of ref. [9] to account for\\nthe vector-baryon channel unitarization (we refer to [9]\\nfor further details):\\nFrom the V V V and BBV interaction Lagrangians pro-\\nvided by the local hidden gauge symmetry formalism\\n[10, 11, 38], the V B →V B tree level potential can be ob-\\ntained, which, neglecting the momentum transfer versus\\n3\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\x01\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\n\\x01\\x01\\x01\\x01\\x01\\nT\\n=\\n+\\n+\\n+\\n. . .\\n+\\nV\\nB i\\nj\\ni\\nj\\nj\\nj\\ni\\ni\\nk\\nl\\nk\\nFIG. 1: Unitarization of the vector-baryon interaction\\nthe baryon mass, is given by\\nVij = −Cij\\n4f 2 (k0\\ni + k0\\nj )⃗ǫi · ⃗ǫj,\\n(1)\\nwhere i (j) stands for the initial (ﬁnal) V B channel, f =\\n93 MeV is the pion decay constant, k0\\ni the energy of the\\nvector meson of the i-th channel and Cij are coeﬃcients\\ngiven by\\nCij =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n2\\n0\\n0\\n3\\n2\\n−1\\n2\\n0\\n0 −\\n√\\n3\\n2 −\\n√\\n3\\n2\\n0\\nq\\n3\\n2\\nq\\n3\\n2\\n0\\n0\\n2\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n,\\n(2)\\nwith Cij = Cji and the indices are in the order ρN, ωN,\\nφN, K∗Λ and K∗Σ.\\nThe chiral unitary approach implements exact unitar-\\nity in coupled channels to obtain the full V B scattering\\nmatrix using the potential of Eq. (1) as the kernel of\\nthe unitarization procedure. Exact unitarity in coupled\\nchannels is implemented by using the Bethe-Salpeter ma-\\ntrix equation (equivalent to the N/D method [39, 40]),\\nwhich has the form\\nT = [1 −V G]−1V\\n(3)\\nwhere Vij is the interaction kernel of Eq. (1) and G is\\na diagonal matrix with Gi the i-th vector-baryon loop\\nfunction, or unitarity bubble. Equivalently, Eq. (3) rep-\\nresents the resummation shown in Fig. 1.\\nThe vector-baryon loop functions Gi need to be regu-\\nlarized which can be accomplished by means of dimen-\\nsional regularization in terms of a subtraction constant\\na(µ) or with a three-momentum cutoﬀqmax. The equiv-\\nalence between both methods was shown in [40, 41]. In\\nref. [9] a subtraction constant a = −2 at a regular-\\nization scale µ = 630 MeV was used, which we have\\nchecked is equivalent to regularizing with a cutoﬀof\\nqmax = 650 MeV.\\nIn order to account for the ﬁnite width of the vector\\nmesons, we fold the loop functions with their correspond-\\ning mass distribution provided by the spectral function\\nρV (sV ),\\nGi(s, MV , MB) = 1\\nN\\nZ (MV +2Γo\\nV )2\\n(MV −2Γo\\nV )2 dsV\\n× ρV (sV ) G(s, √sV , MB),\\n(4)\\n1550\\n1600\\n1650\\n1700\\n1750\\n1800\\n1850\\n1900\\n√\\n__\\n s  [MeV]\\n0\\n1\\n2\\n3\\n4\\n100x |T|\\n2 [MeV\\n-2]\\nρN\\nωN\\nφN\\nK\\n*Λ\\nK\\n*Σ\\nFIG. 2: Diagonal vector-baryon amplitudes without the π∆\\nchannel\\nwhere Γo\\nV is the total width of the vector meson.\\nIn\\nEq. (4), N is the normalization of the spectral distribu-\\ntion:\\nN =\\nZ (MV +2Γo\\nV )2\\n(MV −2Γo\\nV )2 dsV ρV (sV )\\n(5)\\nSince the vector meson mass distributions are Breit-\\nWigner like spectra, we take the spectral function\\nρV (sV ) = −1\\nπ Im\\n\\x1a\\n1\\nsV −M 2\\nV + iMV ΓV (sV )\\n\\x1b\\n,\\n(6)\\nwhere we consider an energy dependent vector meson\\nwidth\\nΓV (sV ) = Γo\\nV\\nM 2\\nV\\nsV\\n\\x12q(√sV )\\nq(MV )\\n\\x133\\nΘ(√sV −\\nq\\nsth\\nV ),\\n(7)\\nwith q(x) the decay momentum of the vector with mass\\nx into the dominant channel, and\\nq\\nsth\\nV the energy of the\\ndominant threshold.\\nThe convolution in Eq. (4) is crucial to get a ﬁnite\\nwidth for the N(1700) resonance since its mass lies below\\nthe lowest V B threshold (in this case ρN) and the width\\nwould otherwise be zero. Therefore the ﬁnite N(1700)\\nwidth comes essentially from the tail of the ρ meson.\\nIn Fig. 2 we show the modulus squared of the ampli-\\ntude of the V B →V B diagonal channels in the region\\naround 1700 MeV. We can see the prominent peak of the\\nN(1700) in the ρN and K∗Λ channels, which has to do\\nwith the couplings to these channels as we will see below.\\nThe triangle singularity in the π∆channel\\nAs mentioned in the introduction, for particular kine-\\nmatic conditions [15, 16], there can be an important en-\\nhancement from a triangular loop for a particular chan-\\nnel which otherwise would be negligible. As we will see,\\n4\\nπ\\nVi\\nBi\\nρ\\nN\\n∆\\nπ\\nq\\nk\\nP\\nFIG. 3: Triangular loop for the π∆channel.\\nthis is the case of the π∆channel in the build up of the\\nN(1700) resonance. Indeed, let us consider the loop dia-\\ngram of Fig. 3.\\nThe value of the incoming energy √s for which the\\ntriangular singularity occurs is given by the solution of\\nthe following equation [16]\\nqon −qa−= 0\\n(8)\\nwhere qon is the on-shell momentum of the nucleon, N,\\nor the ρ meson, in their center of mass frame for a given\\nincoming √s energy, and qa−corresponds to the solution\\nfor ∆→Nπ in the VB rest frame which has negative\\nimaginary part (−iε), with ⃗q and ⃗k antiparallel. (Ex-\\nplicit expressions and further discussion can be found\\nin [16]). In the present case the solution of Eq. (8) is\\n√s = 1715 MeV, just on the N(1700) mass region. This\\nis why one can expect a priori the π∆channel to be\\nrelevant and then its inclusion in the coupled channels\\ndynamics to generate the N(1700) is called for.\\nIn order to include the π∆into the coupled channel\\nequation, Eq. (3), we need the π∆→π∆potential, which\\nis given by Eq. (1) with Cπ∆,π∆= 5 [42], and the non-\\ndiagonal Vi,π∆potentials which can be written as\\nVi,π∆= Vi,ρN Veﬀ\\n(9)\\nwith Veﬀaccounting for the the triangular mechanism\\nin Fig. 3 which we evaluate in the following. For this\\npurpose we need the amplitude for the ρππ interaction\\nand πN∆which we obtain from the usual Lagrangians\\nLV P P = −ig⟨[P, ∂µP]V µ⟩\\nLπN∆= −fπN∆\\nmπ\\nΨ†\\n∆S†\\ni (∂iφλ)T λ†ΨN + h.c.\\n(10)\\nwith g = mρ\\n2f , fπN∆= 2.13 and ⃗S† (⃗T †) the 1/2 to 3/2\\nspin (isospin) transition operators.\\nConsidering the proper isospin 1/2 combination of the\\ninternal ρN states and of the external ∆π states, the\\namplitude of the diagram in Fig. 3 reads\\n−iti = iVi,ρN\\n2\\n√\\n3g fπN∆\\nmπ\\n×2MN\\nZ\\nd4q\\n(2π)4 ⃗ǫi · (2⃗k + ⃗q) ⃗S† · (⃗k + ⃗q)\\n×\\n1\\nq2 −M 2\\nN + iε\\n1\\n(P −q)2 −m2ρ + iε\\n×\\n1\\n(P −q −k)2 −m2π + iε\\n(11)\\nwhich is evaluated in the initial rest frame, ⃗P = 0, and ⃗ǫi\\nstands for the polarization vector of the vector meson in\\nthe initial channel i. The factor 2MN in Eq. (11) stems\\nfrom the use of the Mandl-Shaw normalization [43] for\\nthe fermion ﬁelds. After performing the q0 integration,\\nthe amplitude in Eq. (11) takes the form\\n−iti = Vi,ρN\\n2\\n√\\n3g fπN∆\\nmπ\\n2MN\\n\\x10\\n⃗ǫi · ⃗S† t(1)\\nT\\n+ ⃗ǫi · ⃗k ⃗S† · ⃗k t(2)\\nT\\n\\x11\\n(12)\\nwhere\\nt(α)\\nT\\n=\\nZ\\nd3q\\n(2π)3\\nH(α)\\n8EN ωρ ωπ\\n1\\nk0 −ωπ −ωρ\\n1\\nP 0 −k0 + EN + ωπ\\n×\\n1\\nP 0 −k0 −EN −ωπ + iε\\n1\\nP 0 −EN −ωρ + iε\\n×\\n\\x02\\n2P 0EN + 2k0ωπ −2(EN + ωπ)(EN + ωπ + ωρ)\\n\\x03\\n(13)\\nand\\nH(1) = 1\\n2\\n\"\\n⃗q 2 −(⃗q · ⃗k)2\\n⃗k2\\n#\\n,\\nH(2) = 2 + 3⃗q · ⃗k\\n⃗k2 +\\n1\\n2⃗k2\\n\"\\n3(⃗q · ⃗k)2\\n⃗k2\\n−⃗q 2\\n#\\n.\\n(14)\\nIn Eq. (13), ωρ =\\nq\\nm2ρ + ⃗q 2, ωπ =\\nq\\nm2π + (⃗k + ⃗q)2\\nand EN =\\np\\nM 2\\nN + ⃗q2. Except for the functions H(α),\\nEq. (13) is the triangular amplitude I1 of refs. [16, 44]\\nbut for diﬀerent masses.\\nNote that the previous integrals are logarithmically\\ndivergent and they need to be regularized. For consis-\\ntency with the V B unitarization described in the previ-\\nous section, which needs regularization of the V B loop\\nfor an equivalent cutoﬀof qmax = 650 MeV, we also inte-\\ngrate |⃗q| numerically up to that upper limit. The ﬁnite ρ\\nmeson width can be taken into account by substituting\\nωρ →ωρ −iΓρ/2 in the propagators of Eq. (13).\\nSince it is the core of the present calculation, it is worth\\nplotting numerically the triangular amplitudes t(1)\\nT\\nand\\nt(2)\\nT .\\nThis is depicted in Fig. 4.\\nFor t(1)\\nT\\nwe actually\\n5\\n1550\\n1600\\n1650\\n1700\\n1750\\n1800\\n1850\\n1900\\n√\\n__\\n s  [MeV]\\n-1\\n-0,5\\n0\\n0,5\\n1\\n1,5\\n2\\n2,5\\n3\\n3,5\\n[10\\n-8 MeV\\n-2]\\nRe tT\\n(2)\\nIm tT\\n(2)\\nRe tT\\n(1) /k\\n2\\n Im tT\\n(1) /k\\n2\\nFIG. 4:\\nFunctions t(1)\\nT\\nand t(2)\\nT\\nof the triangle diagram.\\nplot t(1)\\nT /⃗k2 to match dimensions with t(2)\\nT . We can see a\\nlarge enhancement in the imaginary part of t(2)\\nT\\nat about\\n1700 −1750 MeV as expected from the previous discus-\\nsion below Eq. (8), stemming from the triangle singu-\\nlarity. The amplitude t(1)\\nT\\nis not enhanced because the\\nsingularity of the loop is produced at the limits of inte-\\ngration of cos θ = −1 in Eq. (13) (see ref. [15]). Indeed,\\nthe factor H(1) of Eq. (14) goes as 1 −cos2 θ which van-\\nishes precisely at cos θ = −1, canceling the triangle sin-\\ngularity. Therefore, the enhancement due to the triangle\\nsingularity is only manifest in t(2)\\nT .\\nIn the form shown in Eq. (12), that amplitude cannot\\nbe used straightforwardly as the Vi,π∆transition poten-\\ntial of Eq. (9) in the coupled channel equation (3), since\\nit does not have the ⃗ǫi ·⃗ǫj structure of Eq. (1). However,\\nnote that in the coupled channel mechanisms and in order\\nto evaluate a cross section, we would have contributions\\nto a cross section of the kind\\nX\\nM\\n\\x10\\n⃗ǫi · ⃗S t(1)\\nT\\n+ ⃗ǫj · ⃗k ⃗S · ⃗k t(2)\\nT\\n\\x11\\n|M⟩⟨M|\\n\\x10\\n⃗ǫj · ⃗S† t(1)\\nT\\n∗\\n+ ⃗ǫj · ⃗k ⃗S† · ⃗k t(2)\\nT\\n∗\\x11\\n(15)\\nwith M the third component of the ∆spin. Neglecting\\nsome spin-ﬂip terms, not of the ⃗ǫi ·⃗ǫj form, Eq. (15) gives\\n\\x122\\n9\\n⃗k4t(2)\\nT t(2)\\nT\\n∗+ 2\\n9\\n⃗k2t(2)\\nT t(1)\\nT\\n∗\\n+2\\n9\\n⃗k2t(1)\\nT t(2)\\nT\\n∗+ 2\\n3t(1)\\nT t(1)\\nT\\n∗\\x13\\n⃗ǫi · ⃗ǫj\\n(16)\\nAnd then we conclude that Veﬀof Eq. (9) is such that\\nV 2\\neﬀ=\\n\\x12 2\\n√\\n3g fπN∆\\nmπ\\n2MN\\n\\x132 \\x122\\n9\\n⃗k4t(2)\\nT t(2)\\nT\\n∗\\n+2\\n9\\n⃗k2t(2)\\nT t(1)\\nT\\n∗+ 2\\n9\\n⃗k2t(1)\\nT t(2)\\nT\\n∗+ 2\\n3t(1)\\nT t(1)\\nT\\n∗\\x13\\n(17)\\nwith |⃗k| the on-shell π momentum of the π∆system with\\nenergy √s.\\nSo far we have not considered the ﬁnite ∆width which\\nmay be relevant because it is large, about 120 MeV. This\\neﬀect can be easily taken into account, on one hand, by\\nfolding the π∆loop function with an analogous expres-\\nsion to Eq. (4) but integrating for √s∆and using the\\nspectral function\\nρ∆(s∆) = −1\\nπ Im\\n(\\n1\\n√s∆−M∆+ i Γ∆\\n2\\n)\\n,\\n(18)\\nand, on the other hand, by folding Eq. (17) by this same\\n∆spectral function.\\nOn the other hand, Eq. (12) and Eq. (15) account both\\nfor π∆S- and D-waves. However, we are also interested\\nin singling out the S-wave contribution to compare the S-\\nwave partial decay width into π∆with the corresponding\\ncolumn in Table I. In order to do this we can trivially\\nrewrite the ⃗ǫ · ⃗k ⃗S · ⃗k structure of Eq. (12) as\\n⃗ǫ · ⃗k ⃗S · ⃗k = Siǫjkikj =\\n= Siǫj\\n\\x14\\x12\\nkikj −1\\n3\\n⃗k2δij\\n\\x13\\n+ 1\\n3\\n⃗k2δij\\n\\x15\\n.\\n(19)\\nNote that the kikj −1\\n3⃗k2δij piece is purely D-wave and\\n1\\n3⃗k2δij is purely S-wave. Therefore, if we keep only the\\nS-wave part we arrive to an equivalent expression to\\nEq. (17) but which accounts only for the the S-wave π∆\\ncontribution:\\nV 2\\neﬀ(S-wave) =\\n\\x12 2\\n√\\n3g fπN∆\\nmπ\\n2MN\\n\\x132 2\\n3\\n\\x0c\\x0c\\x0c\\x0ct(1)\\nT\\n+ 1\\n3\\n⃗k2t(2)\\nT\\n\\x0c\\x0c\\x0c\\x0c\\n2\\n.\\n(20)\\nThe N(1700) couplings to V B, π∆, and partial decay\\nwidths\\nUsually the couplings of the resonance to the diﬀerent\\nchannels would be obtained from the residues of the am-\\nplitudes to the diﬀerent channels in the second Riemann\\nsheet. However, in the present case the N(1700) is very\\nclose to the ρN threshold and furthermore the ρ width\\ndistorts the shape of the amplitude. In such a case it\\nis much better and well deﬁned to obtain the couplings\\nfrom the amplitudes in the real axis, which actually con-\\ntain the meaningful information of the resonance.\\nAs we can see in Fig. 2 and we will also see below (see\\nFig. 6), the amplitudes resemble pretty much a Breit-\\nWigner and thus we can parametrize the amplitudes\\naround the resonance mass position as\\nTij(√s) =\\ngigj\\n√s −MR + i ΓR\\n2\\n,\\n(21)\\n6\\nwhere R stands for the N(1700) resonance, and gi is the\\ncoupling of the N(1700) to a V B or π∆, from where the\\ndiﬀerent couplings are related by\\ngi = gj\\nTij(MR)\\nTjj(MR)\\n(22)\\nwhich allows to obtain all the couplings from one coupling\\ngj which, up to a global sign, can be obtained from\\ng2\\nj = iΓR\\n2 Tjj(MR)\\n(23)\\nwhich we choose to be gj = gρN, and MR is the position\\nof the peak of |Tjj|2. From the values of the couplings\\nwe can evaluate the partial decay widths of the N(1700)\\nto the diﬀerent V B and π∆channels:\\nΓi(MR, mV ) = 1\\n2π\\nMi\\nMR\\n|gi|2q\\n(24)\\nwith\\nq\\nthe\\ndecay\\nmomentum\\nand\\nMi(mV )\\nthe\\nbaryon(vector) mass.\\nExcept for the π∆channel, the\\nthresholds are above the N(1700) mass and thus there is\\nno phase space for the decay to happen if the resonances\\nwere narrow. In those cases the decay takes place thanks\\nto the width of the vector meson and the N(1700), there-\\nfore we fold the expression for the width with the mass\\ndistribution of the broad particles involved as\\nΓR→i = 1\\nN\\nZ (MR+ΓR)2\\n(MR−ΓR)2 dsR\\nZ (MV +2ΓV )2\\n(MV −2ΓV )2 dsV\\n×ρR(sR) ρV (sV ) Γi(√sR, √sV )Θ(√sR −√sV −Mi)\\n(25)\\nwhere\\nΘ\\nis\\nthe\\nstep\\nfunction,\\nwith\\nq\\n=\\n1\\n2√sR λ1/2(sR, sV , M 2\\ni ) and\\nN =\\nZ (MR+ΓR)2\\n(MR−ΓR)2 dsR\\nZ (MV +2ΓV )2\\n(MV −2ΓV )2 dsV ρR(sR) ρV (sV ).\\n(26)\\nFor the π∆channel the expression is analogous but fold-\\ning with the ∆spectral function instead of the vector-\\nmeson one.\\nRESULTS\\nFirst we show in Fig. 5 the value of the transition po-\\ntentials from the diﬀerent channels to the dominant one,\\nρN, in order to foresee the importance of the π∆chan-\\nnel in the build up of the N(1700). The transitions to\\nωN and φN are zero (see Eq. (2)). We see in that ﬁgure\\nthat the ρN →π∆potential is about 40% the size of the\\nρN →ρN one at the N(1700) peak, which anticipates an\\nimportant contribution of the ∆π channel in the build up\\n1550\\n1600\\n1650\\n1700\\n1750\\n1800\\n1850\\n1900\\n√\\n__\\n s  [MeV]\\n-0,1\\n-0,05\\n0\\nVρN,i  [MeV\\n-1]\\nVρN,ρN \\nVρN,K\\n*Λ\\nVρN,K\\n*Σ\\nVρN,π∆\\nFIG. 5: Transition potentials to ρN.\\n1550\\n1600\\n1650\\n1700\\n1750\\n1800\\n1850\\n1900\\n√\\n__\\n s  [MeV]\\n0\\n1\\n2\\n3\\n4\\n100x |T|\\n2 [MeV\\n-2]\\nTρN,ρN  (w/o π∆)\\nTρN,ρN  (with π∆)\\nTρN,π∆\\nFIG. 6: Full amplitudes with and without the π∆channel.\\nof the resonance. It is worth noting that the resonance-\\nlike shape from the triangle singularity that we can see\\nin Fig. 4 is distorted in VρN,π∆due to the ⃗k2 factors in\\nEq. (17). Nonetheless, it is not the shape of the potential\\nwhat matters to give the N(1700) resonance shape but\\nits strength, since the resonance comes afterwards from\\nthe non-linear coupled channel dynamics.\\nIn Fig. 6 we show the modulus squared of TρN,ρN,\\nwith and without considering the eﬀect of the π∆chan-\\nnel, and of TρN,π∆.\\nFor the other channels the eﬀect\\nof considering π∆is similar but we only show ρN for\\nclarity and because ρN is the dominant one.\\nWe can\\nsee that, although the TρN,π∆amplitude is small, the\\neﬀect of the π∆channel in the highly non-linear dynam-\\nics involved in the coupled channels produces a signiﬁ-\\ncant broadening of the N(1700) resonance. Actually, by\\nﬁtting to a Breit-Wigner shape, the mass and width of\\nthe N(1700) resonance before considering the π∆chan-\\nnel was MR = 1701 MeV, ΓR = 105 MeV, and after\\nincluding the π∆channel they are MR = 1699 MeV,\\nΓR = 141 MeV.\\nWhile the mass barely changes, the\\n7\\nwidth increases about 35% from the inclusion of the π∆\\nchannel.\\nAs can be seen in Table I, the width of the\\nN(1700) resonance in the PDG [1] is quite uncertain\\n100 −250 MeV, and it seems that a large width is pre-\\nferred, for which the inclusion of the π∆channel goes in\\nthe right direction.\\nIn Table II we show the couplings and partial decay\\nwidths, with and without including the π∆channel.\\nThe decay width to π∆is relevant despite the small\\nvalue of its coupling because of the larger phase space\\navailable for this channel. Note that the partial decay\\nwidths do not sum up exactly to the total width of the\\nN(1700) resonance since the convolution we are carrying\\nout to get the V B partial decay widths is just an ap-\\nproximation and the resonance is very close to the ρN\\nthreshold.\\nHowever, the sum of partial decay widths,\\n129 MeV, is close to the total width of 141 MeV. This\\ndiscrepancy should be understood as a measure of the\\nuncertainty in our calculation of the decay widths.\\nIt is worth mentioning that the analyses [2, 5] that get\\na large branching ratio to π∆in Table I, ﬁt data with\\nππN in the ﬁnal state but only for two neutral π0, and\\nthe ρ meson does not couple to π0π0.\\nTherefore they\\ndo not have the ρN channel in their analyses and thus\\nit is easy to overestimate the weight of the π∆. How-\\never in the analysis of ref. [6], ππN data with charged\\npions are also considered and thus the ρN weight is also\\nobtained. When the ρN channel is considered, the im-\\nportance of the π∆is reduced considerably in the direc-\\ntion of what we get in the present work. Furthermore\\nnote that, as already mentioned in the introduction, the\\nmass obtained in refs. [2, 5] for the N(1700) resonance\\nlays around 1800 MeV and the width is also much larger\\nthan in the other analyses. This could be an indication\\nthat the state around 1700 MeV and the one around 1800\\nMeV are actually two diﬀerent resonances and thus the\\ncoupling to π∆could be very diﬀerent.\\nIn this context it is interesting to note that the anal-\\nysis of Ref. [6], which gives a sizeable fraction of ρN\\ndecay, 38 ± 6%, also provides a small mass for the reso-\\nnance, and one should accept that this channel, being\\nonly allowed because of the width of the ρ meson, is\\nnot particularly easy to determine quantitatively, which\\nmeans that part of its strength can be easily attributed\\nto other channels. The results of our study clearly in-\\ndicate a large dominance of ρN in the build up of the\\nresonance and also in its decay width.\\nHowever, it is\\nalso clear that the π∆channel has a signiﬁcant contri-\\nbution which theoretically we can only explain from the\\ntriangular singularity studied in the present work. These\\nﬁndings, and the large dispersion of the experimental re-\\nsults concerning the N(1700), should serve to stimulate\\nfurther experimental work to the light of the present re-\\nsults. This should also include some modiﬁcation in the\\nanalysis tools to explicitly include the structure tied to\\nthe triangle singularity, which, as we have discussed here,\\nis tied to the masses of the particles in the triangle dia-\\ngram and, hence, is unavoidable.\\nSUMMARY AND CONCLUSIONS\\nIn the present work we have shown how a channel,\\nwhich otherwise would be negligible, can gain unsus-\\npected importance in the build up of a resonance thanks\\nto an enhancement in a triangular loop from a mathemat-\\nical singularity appearing for particular kinematic condi-\\ntions. Speciﬁcally we show that the π∆channel has an\\nimportant relevance in the N(1700) formation of the res-\\nonance because of a singularity, of non-resonant nature,\\nin the Feynman diagram of Fig. 3. We have shown how\\nthe N(1700) is dynamically generated from the imple-\\nmentation of unitarity from the interaction of the octet\\nof vector mesons and the octet of baryons in coupled\\nchannels and how the V B can couple eﬀectively to π∆\\nthrough the triangle diagram with a ρ meson, a nucleon\\nand a pion in the internal lines. For the values of the\\nmasses of the particles involved, the loop has a singular-\\nity at about the same energy where the N(1700) peaks.\\nThis makes the π∆channel gain an importance which\\notherwise would be negligible. We obtain an increase of\\nabout 35% in the width of the N(1700) resonance when\\nthe π∆channel is taken into account and make predic-\\ntions for the partial decay widths into the diﬀerent V B\\nchannels and the π∆channel, considering also the S-wave\\nseparately in this latter channel.\\nWe stressed the fact that presently there is a large dis-\\npersion in the data of the diﬀerent analyses, and, hence,\\na quantitative comparison with our results would be in-\\nconclusive. Instead, we pointed out at the convenience of\\na reanalysis of the data to the light of the ﬁndings of the\\npresent work, which include the unavoidable presence of\\na triangle singularity. To account for it, a modiﬁcation of\\nthe tools normally used in experimental analyses would\\nhave to be implemented, to include the structure of the\\ntriangle mechanism as an extra multiplicative factor in\\nthe π∆channel. The continuous accumulation of data\\nin the ππN channel, and improved analysis tools, should\\nbring more precise information on this resonance in the\\nfuture, allowing us to learn more about its nature.\\nAcknowledgments\\nThis work is partly supported by the Spanish Min-\\nisterio de Economia y Competitividad and European\\nFEDER funds under the contract number FIS2011-\\n28853-C02-01, FIS2011- 28853-C02-02, FIS2014-57026-\\nREDT, FIS2014-51948-C2- 1-P, and FIS2014-51948-C2-\\n2-P, and the Generalitat Valenciana in the program\\nPrometeo II-2014/068 (EO).\\n8\\nwithout π∆\\nMR = 1701\\nΓR = 104\\nwith π∆\\nMR = 1698\\nΓR = 141\\ngi\\n|gi|\\nΓi\\ngi\\n|gi|\\nΓi\\nΓi/ΓR\\nρN\\n3.07 −1.09i\\n3.26 108\\n2.98 −1.35i\\n3.27 114\\n80%\\nωN\\n0.17 −0.06i\\n0.18 0.2\\n0.17 −0.08i\\n0.19 0.3\\n0.2%\\nφN\\n−0.26 + 0.09i 0.28\\n0\\n−0.26 + 0.12i 0.28\\n0\\n0\\nK∗Λ\\n2.61 −0.92i\\n2.77\\n0\\n2.55 −1.15i\\n2.80 0.1\\n0.1%\\nK∗Σ\\n−0.63 + 0.22i 0.67\\n0\\n−0.61 + 0.28i 0.67\\n0\\n0\\nπ∆\\n–\\n–\\n–\\n0.16 + 0.57i\\n0.61 14.6\\n10%\\nπ∆\\n(S-wave)\\n–\\n–\\n–\\n−0.28 + i0.26 0.40 6.4\\n5%\\nTABLE II: Couplings and partial decay widths of the N(1700) resonance. All dimensional magnitudes are in MeV.\\n[1] C. Patrignani et al. (Particle Data Group), Chin. Phys.\\nC, 40, 100001 (2016).\\n[2] V. Sokhoyan et al. [CBELSA/TAPS Collaboration], Eur.\\nPhys. J. A 51, no. 8, 95 (2015) Erratum: [Eur. Phys. J.\\nA 51, no. 12, 187 (2015)].\\n[3] R. E. Cutkosky, C. P. Forsyth, R. E. Hendrick and\\nR. L. Kelly, Phys. Rev. D 20, 2839 (1979).\\n[4] G. Hohler, πN Newsletter 9, 1 (1993)\\n[5] A. V. Anisovich, R. Beck, E. Klempt, V. A. Nikonov,\\nA. V. Sarantsev and U. Thoma, Eur. Phys. J. A 48, 15\\n(2012).\\n[6] M. Shrestha and D. M. Manley, Phys. Rev. C 86, 055203\\n(2012).\\n[7] M. Batinic, S. Ceci, A. Svarc and B. Zauner, Phys. Rev.\\nC 82, 038203 (2010).\\n[8] T. P. Vrana, S. A. Dytman and T. S. H. Lee, Phys. Rept.\\n328, 181 (2000).\\n[9] E. Oset and A. Ramos, Eur. Phys. J. A 44, 445 (2010).\\n[10] M. Bando, T. Kugo, S. Uehara, K. Yamawaki and\\nT. Yanagida, Phys. Rev. Lett. 54, 1215 (1985).\\n[11] M. Bando, T. Kugo and K. Yamawaki, Phys. Rept. 164,\\n217 (1988).\\n[12] U. G. Meissner, Phys. Rept. 161, 213 (1988).\\n[13] L. D. Landau, Nucl. Phys. 13 (1959) 181.\\n[14] X. H. Liu, M. Oka and Q. Zhao, Phys. Lett. B 753 (2016)\\n297.\\n[15] S. Coleman and R. E. Norton, Nuovo Cim. 38 (1965)\\n438.\\n[16] M. Bayar, F. Aceti, F. K. Guo and E. Oset, Phys. Rev.\\nD 94 (2016) no.7, 074039.\\n[17] J. J. Wu, X. H. Liu, Q. Zhao and B. S. Zou, Phys. Rev.\\nLett. 108 (2012) 081803.\\n[18] F. Aceti, W. H. Liang, E. Oset, J. J. Wu and B. S. Zou,\\nPhys. Rev. D 86 (2012) 114007.\\n[19] X. G. Wu, J. J. Wu, Q. Zhao and B. S. Zou, Phys. Rev.\\nD 87 (2013) no.1, 014023.\\n[20] C. Adolph et al. [COMPASS Collaboration], Phys. Rev.\\nLett. 115 (2015) no.8, 082001.\\n[21] M. Mikhasenko, B. Ketzer and A. Sarantsev, Phys. Rev.\\nD 91 (2015) no.9, 094015.\\n[22] F. Aceti, L. R. Dai and E. Oset, Phys. Rev. D 94 (2016)\\nno.9, 096015.\\n[23] V. R. Debastiani, F. Aceti, W. H. Liang and E. Oset,\\narXiv:1611.05383 [hep-ph].\\n[24] E. Wang, J. J. Xie, W. H. Liang, F. K. Guo and E. Oset,\\nPhys. Rev. C 95, no. 1, 015205 (2017).\\n[25] E. Gutz et al. [CBELSA/TAPS Collaboration], Eur.\\nPhys. J. A 50, 74 (2014).\\n[26] V. R. Debastiani, S. Sakai and E. Oset, (To be submitted)\\n[27] J. J. Xie, L. S. Geng and E. Oset, arXiv:1610.09592 [hep-\\nph].\\n[28] Q. Wang, C. Hanhart and Q. Zhao, Phys. Lett. B 725\\n(2013) no.1, 106.\\n[29] N. N. Achasov, A. A. Kozhevnikov and G. N. Shestakov,\\nPhys. Rev. D 92 (2015) no.3, 036003.\\n[30] I. T. Lorenz, H. W. Hammer and U. G. Meissner, Phys.\\nRev. D 92 (2015) no.3, 034018.\\n[31] A. P. Szczepaniak, Phys. Lett. B 747 (2015) 410.\\n[32] A. P. Szczepaniak, Phys. Lett. B 757 (2016) 61.\\n[33] D. Samart, W.H. Liang and E. Oset, (To be submitted)\\n[34] R. Aaij et al. [LHCb Collaboration], Phys. Rev. Lett. 115\\n(2015) 072001.\\n[35] R. Aaij et al. [LHCb Collaboration], Chin. Phys. C 40\\n(2016) no.1, 011001.\\n[36] F. K. Guo, U. G. Meissner, W. Wang and Z. Yang, Phys.\\nRev. D 92 (2015) no.7, 071502.\\n[37] X. H. Liu, Q. Wang and Q. Zhao, Phys. Lett. B 757\\n(2016) 231.\\n[38] H. Nagahiro, L. Roca, A. Hosaka and E. Oset, Phys. Rev.\\nD 79 (2009) 014015.\\n[39] J. A. Oller and E. Oset, Phys. Rev. D 60 (1999) 074023.\\n[40] J. A. Oller and U. G. Meißner, Phys. Lett. B 500, 263\\n(2001).\\n[41] J. A. Oller, E. Oset and J. R. Pelaez, Phys. Rev. D\\n59, 074001 (1999) [Erratum-ibid. D 60, 099906 (1999)]\\n[Erratum-ibid. D 75, 099903 (2007)].\\n[42] S. Sarkar, B. X. Sun, E. Oset and M. J. Vicente Vacas,\\nEur. Phys. J. A 44 (2010) 431\\n[43] F. Mandl and G. Shaw, Quantum Field Theory, Wiley-\\nInterscience, 1984.\\n[44] F. Aceti, J. M. Dias and E. Oset, Eur. Phys. J. A 51\\n(2015) no.4, 48.\\n'),\n",
       " Document(metadata={'Published': '2006-05-02', 'Title': 'Clues to the nature of the Delta^*(1700) resonance from pion- and photon-induced reactions', 'Authors': 'M. Doring, E. Oset, D. Strottman', 'Summary': 'We make a study of the (pi^- p --> K^0 pi^0 Lambda), (pi^+ p --> K^+ pi^+\\nLambda), (K^+\\\\bar{K}^0 p), (K^+ pi^+ Sigma^0), (K^+ pi^0 Sigma^+), and (eta\\npi^+ p) reactions, in which the basic dynamics is given by the excitation of\\nthe Delta^*(1700) resonance which subsequently decays into (K Sigma^*(1385)) or\\n(Delta(1232) eta). In a similar way we also study the (gamma p --> K^0 pi^+\\nLambda), (K^+ pi^- Sigma^+), (K^+ pi^+ Sigma^-), (K^0 pi^0 Sigma^+), and (eta\\npi^0 p) related reactions. The cross sections are proportional to the square of\\nthe coupling of Delta^*(1700) to (Sigma^*K), (Delta eta) for which there is no\\nexperimental information but which is provided in the context of coupled\\nchannels chiral unitary theory where the Delta^*(1700) is dynamically\\ngenerated. Within present theoretical and experimental uncertainties one can\\nclaim a global qualitative agreement between theory and experiment. We provide\\na list of items which need to be improved in order to make further progress\\nalong these lines.'}, page_content='arXiv:nucl-th/0602055v2  2 May 2006\\nClues to the nature of the ∆∗(1700) resonance from pion- and photon-induced reactions\\nM. D¨oring,∗E. Oset,† and D. Strottman‡\\nDepartamento de F´ısica Te´orica and IFIC, Centro Mixto Universidad de Valencia-CSIC,\\nInstitutos de Investigaci´on de Paterna, Aptd. 22085, 46071 Valencia, Spain\\nWe make a study of the π−p →K0π0Λ, π+p →K+π+Λ, K+ ¯K0p, K+π+Σ0, K+π0Σ+, and ηπ+p\\nreactions, in which the basic dynamics is given by the excitation of the ∆∗(1700) resonance which\\nsubsequently decays into KΣ∗(1385) or ∆(1232)η. In a similar way we also study the γp →K0π+Λ,\\nK+π−Σ+, K+π+Σ−, K0π0Σ+, and ηπ0p related reactions. The cross sections are proportional to\\nthe square of the coupling of ∆∗(1700) to Σ∗K (∆η) for which there is no experimental information\\nbut which is provided in the context of coupled channels chiral unitary theory where the ∆∗(1700)\\nis dynamically generated. Within present theoretical and experimental uncertainties one can claim\\na global qualitative agreement between theory and experiment. We provide a list of items which\\nneed to be improved in order to make further progress along these lines.\\nPACS numbers: 24.10.Eq, 14.20.Gk, 13.75.-n\\nI.\\nINTRODUCTION\\nThe history of the dynamically generated resonances, which appear in the solution of the meson-meson or meson-\\nbaryon coupled channel Lippmann-Schwinger equation (LSE) with some interaction potential, is quite old. One of\\nthe typical examples is the Λ(1405) resonance which appears naturally in coupled channels containing the πΣ and\\nKN channels [1, 2]. The advent of unitary extensions of chiral perturbation theory has brought more systematics into\\nthis approach with chiral Lagrangians providing the kernel, or potential, for the LSE or its relativistic counterpart,\\nthe Bethe Salpeter equation (BSE) which is more often used. In this sense the Λ(1405) has been revisited from this\\nnew perspective and at the same time new resonances like the N ∗(1535), Λ(1670), etc. have been claimed to be also\\ndynamically generated [3, 4, 5, 6, 7, 8, 9, 10]. Actually, one of the surprises along these lines was the realization that\\nthe chiral theory predicted the presence of two nearby I = 0, S = −1 poles close to the nominal Λ(1405) mass, such\\nthat the physical resonance would be a superposition of the two states [11, 12]. Recent work including also the eﬀect\\nof higher order Lagrangians in the kernel of the BSE [13, 14, 15] also ﬁnd two poles in that channel, one of them with\\na large width [13]. Interestingly, recent measurements of the K−p →π0π0Σ0 reaction [16] show the excitation of the\\nΛ(1405) in the π0Σ0 invariant mass, peaking around 1420 MeV and with a smaller width than the nominal one of the\\nPDG [17]. An analysis of this reaction and comparison with the data of [18] from the π−p →K0πΣ reaction led the\\nauthors of Ref. [19] to conclude that the combined experimental information of these two reactions provided evidence\\nof the existence of two Λ(1405) states.\\nMore recent work has extended the number of dynamically generated resonances to the low lying 3/2−resonances\\nwhich appear from the interaction of the octet of pseudoscalar mesons with the decuplet of baryons [20, 21]. One\\nof the resonances that appears qualitatively as dynamically generated is the Λ(1520), built up from πΣ∗(1385) and\\nKΞ∗(1530) channels, although the necessary coupling to the πΣ and KN channels makes the picture more complicated\\n[22, 23]. A more detailed discussion on the meaning of the dynamically generated resonances, its physical interpretation\\np\\nγ\\n∆∗\\nΣ∗+\\nK0\\nπ0\\nΣ+\\nFIG. 1: Tree level contribution for the γp →K0π0Σ+ reaction.\\n∗Electronic address: doering@iﬁc.uv.es\\n†Electronic address: oset@iﬁc.uv.es\\n‡Electronic address: dds@iﬁc.uv.es\\n2\\nand the technical details on how to produce them can be seen in Sec. II of the paper [24].\\nAnother 3/2−resonance which appears in the same scheme is the ∆∗(1700), and in [21] the couplings of the resonance\\nto the coupled channels ∆π, Σ∗K, and ∆η were calculated. The couplings gi are 1.0, 3.4, and 2.2, respectively, for\\nthese channels. It is interesting to note the large strength of the coupling to the Σ∗K channel. Due to this, it was\\nfound in Ref. [25] that the γp →K0π0Σ+ reaction was dominated by the mechanisms shown in Fig. 1 where the\\n∆∗(1700) is excited by the photon and decays into K0Σ∗+ and the Σ∗+ subsequently decays into π0Σ+. Since the cross\\nsection for this reaction is proportional to (g∆∗KΣ∗)2, the agreement of the predicted cross section with experiment\\nwould provide support for the coupling provided by the theory with the assumption of the ∆∗(1700) as a dynamically\\ngenerated resonance. The experiment for this reaction has been performed at ELSA and is presently under analysis.\\nPreliminary results presented in the NSTAR05 workshop [26] agree with the theoretical predictions.\\nWe would like to stress the fact that the couplings predicted by the theory based on the dynamical nature of\\nthe ∆∗(1700) resonance are by no means trivial. Indeed, should we assume that the ∆∗(1700) belongs to an SU(3)\\ndecuplet as suggested in the PDG (table 14.5) [17] it is easy to see that the couplings to the ∆π, Σ∗K, ∆η states in\\nI = 3/2 are proportional to\\np\\n5/8,\\np\\n1/4,\\np\\n1/8, respectively. The squares of these coeﬃcients are proportional to 1,\\n2/5, 1/5, respectively, compared to the squares of the coeﬃcients of the dynamically generated resonance, 1, 11.56,\\n4.84. It was noted in [21] that the strength of the ∆π coupling of the dynamically generated model was consistent with\\nthe experimental branching ratio of the ∆∗(1700) . Hence, this means that the dynamically generated model produces\\nconsiderable strength for the Σ∗K and ∆η channels in absolute terms. With respect to the decuplet assumption of the\\nPDG one obtains factors 27.5 and 24 larger for the square of the couplings to Σ∗K and ∆η, respectively. These large\\ncouplings indicate that, even if the ∆∗(1700) resonance is somewhat sub-threshold for the πN →Σ∗K and πN →∆η\\nreactions, the combination of these couplings and the ∆∗(1700) width (∼300 MeV) should make this resonance play\\nan important role in those reactions close to their thresholds.\\nIt is clear that ultimately it is the consistency of the predictions of the theoretical models that builds up support\\nfor the theory. Hence, it is straightforward to suggest an additional reaction with a similar mechanism as in Fig. 1\\nbut rather with the Σ∗→πΛ decay. Since the branching ratio for πΛ decay of the Σ∗is 88%, the cross section would\\nbe reasonably larger than for γp →K0π0Σ+ and one would have extra tests for the model.\\nAdditional tests can be also done with the related reactions, π−p →K0π0Λ and all the other pion-induced reactions\\nmentioned in the abstract, for which some data on cross sections are already available [18, 27, 28, 29, 30]. It is quite\\ninteresting to recall that in the theoretical model studied in [31], which was based on the excitation of the Λ(1405),\\nthe K0K−p, K0K\\n0n, K0π+Σ−, and K0π−Σ+ channels were reproduced within 25%, while the cross section for\\nthe K0π0Λ channel predicted was 6 µb compared to the 104 ± 8 µb of the experiment. The lack of the Σ∗(1385)\\nresonance in the model of [31], which relied upon the ﬁnal state interaction of the particles to generate dynamically\\nthe resonances, did not allow one to make a realistic approach for the K0π0Λ ﬁnal state, which in [18] was shown to\\nbe dominated by K0Σ∗0 production. The Σ∗(1385), as all the other elements of the decuplet of the ∆(1232), does\\nnot qualify as a dynamically generated resonance, and is indeed a building block to generate other resonances like the\\nΛ∗(1520) or ∆∗(1700).\\nWhile at the time of [31] the information on the ∆∗(1700) →KΣ∗coupling was not available, the works of [20, 21],\\nand particularly [21] where the coupling is evaluated, have opened the door to tackle this reaction and this is one of\\nthe aims of the present work.\\nThe data for the π−p →K0π0Λ reaction from [18] is at √s = 2020 MeV which is about 320 MeV above the\\n∆∗(1700) peak, potentially too far away to claim dominance of the ∆∗(1700), but there are also data at lower energies\\n[27, 28] around √s = 1930-1980 MeV. On the other hand there are data [29, 30] for other reactions, π+p →K+π+Λ,\\nπ+p →K+π+Σ0, π+p →K+π0Σ+, and π+p →ηπ+p at energies around √s = 1800 MeV which allow us to make\\na more direct comparison with the theoretical predictions. In the next section we study these reactions. Similarly,\\nthere are also some data for the γp →K0π+Λ, K+π−Σ+, K+π+Σ−reactions [32, 33, 34] and we shall also address\\nthem in the same context.\\nII.\\nTHE MODEL FOR THE πp →KπΛ, KπΣ, K ¯KN, ηπN REACTIONS\\nIn the work of [25] on the γp →K0π0Σ+ reaction, the diagram of Fig. 1 was evaluated together with many loop\\ndiagrams involving rescattering of the KΣ state which are the building blocks together with πN and others of the\\nN ∗(1535) resonance. It was found there that the tree level diagram was dominant. For energies of √s = 2000 MeV,\\nhence 300 MeV above the ∆∗(1700) nominal mass, the loop terms could provide a contribution of about 30% with large\\nuncertainties. On the other hand the results obtained for the cross section ﬁnd support in preliminary experimental\\nresults presented at the NSTAR05 workshop [26].\\nWith this information at hand we have good justiﬁcation to\\npropose that the dominant mechanism for the π−p →K0π0Λ, π+p →K+π+Λ, K+ ¯K0p, K+π+Σ0, K+π0Σ+, and\\n3\\np\\nπ−\\n∆∗0(1700)\\nΣ∗0(1385)\\nK0\\nΛ\\nπ0\\np\\nπ+\\n∆∗++(1700) Σ∗+(1385)\\nK+\\nΛ\\nπ+\\np\\nπ+\\n∆∗++(1700) Σ∗+(1385)\\nK+\\np\\n¯K0\\np\\nπ+\\n∆∗++(1700) Σ∗+(1385)\\nK+\\nΣ0(+)\\nπ+(0)\\np\\nπ+\\n∆∗++(1700)∆∗++(1232)\\nη\\np\\nπ+\\nFIG. 2: Tree level contributions for the pion-induced strangeness production via the ∆∗(1700) .\\nηπ+p reactions is given by the diagrams of Fig. 2.\\nAll the elements of these diagrams are at hand from the works of [21, 25]. The new information needed here is the\\nπN coupling to the ∆∗(1700) which is not an ingredient of the building blocks in the studies of [20, 21] that only take\\ninto account the interaction of the octet of pseudoscalar mesons with the decuplet of baryons. Thus, we take this\\ninformation from experiment by looking at the branching ratio in the PDG [17]. In spite of the larger phase space for\\ndecay into this channel the branching ratio to πN is only 10-20%.\\nBy taking into account that the coupling of πN to ∆∗(1700)(3/2−) is in d-wave, the structure of the ∆∗πN vertex\\nis most conveniently written as\\n−it∆∗(1700)→πN = −ig(d)\\nπN∆∗C(1/2 2 3/2; m, M −m) Y ∗\\n2, m−M(ˆk)(−1)M−m√\\n4π\\n(1)\\nas in Ref. [22] (see their Eq. (25)) to account for the Λ∗(1520) →πΣ coupling. In Eq. (1) the Clebsch Gordan\\ncoeﬃcient accounts for the matrix element of the rank two spin operator needed to couple the spherical harmonic\\nY2 to a scalar. The quantities M, m are the third components of the spin of the ∆∗and the nucleon and k is the\\nmomentum of the pion. The πN state is in I = 3/2.\\nWith the parametrization of Eq. (1) the partial decay width of the ∆∗to πN is written as\\nΓ∆∗→πN = (g(d)\\nπN∆∗)2\\n2π\\nMN\\nM∆∗kπ.\\n(2)\\nBy taking the width of the ∆∗, Γ∆∗= 300 ± 100 MeV, and the πN branching ratio of 15 ± 5% and summing the\\nerrors in quadrature we obtain the value\\ng(d)\\nπN∆∗= 0.94 ± 0.20\\n(3)\\nwhich will necessarily lead to uncertainties of the order of 50% in the cross section. Given the isospin decomposition\\nof the π−p state in I = 1/2, 3/2, the coupling gπN∆∗(in I = 3/2) has to be multiplied by\\np\\n1/3 to account for the\\ncoupling of the π−p state to the ∆∗and by −1 to account for the coupling of π+p to ∆∗++ (although irrelevant for\\nthe cross section we use the isospin phase convention |π+⟩= −|1, 1⟩). This means\\ng(d)\\nπ−p∆∗0 =\\nr\\n1\\n3 g(d)\\nπN∆∗, g(d)\\nπ+p∆∗++ = −g(d)\\nπN∆∗.\\n(4)\\nThere is another point worth noting which is that due to the d-wave character of the πN∆∗vertex, the coupling\\ngπN∆∗implicitly incorporates k2\\nπ for the on-shell value of the pion momentum in the ∆∗→πN decay. When we\\nextrapolate beyond the resonance energy, as will be the case here, we must then use\\ngπN∆∗→gπN∆∗(on shell) BW(kπ R)\\nBW(kon\\nπ R)\\n(5)\\n4\\nwhere BW(·) is the Blatt and Weisskopﬀpenetration factor [35, 36, 37]\\nBW(x) =\\nx2\\n(9 + 3x2 + x4)1/2\\n(6)\\nand R = 0.4 fm according to best ﬁts of [36].\\nThe other couplings needed are those of the decuplet to the meson and baryon octets. For the vertices with Σ∗(1385)\\nor ∆(1232) decay in the diagrams of Fig. 2 we use the chiral Lagrangian [38]\\nL = C\\n\\uf8eb\\n\\uf8ed\\n1,··· ,3\\nX\\na,b,c,d,e\\nǫabc T\\nade uµ Ab,µ\\nd\\nBc\\ne +\\n1,··· ,3\\nX\\na,b,c,d,e\\nǫabc B\\ne\\nc Ad\\nb,µ Tade uµ\\n\\uf8f6\\n\\uf8f8.\\n(7)\\nThe Rarita-Schwinger ﬁelds u are deﬁned as in [21] and the ﬂavor tensor T is given in [38]. In Eq. (7), the quantity\\nAµ, which is proportional to the axial current, is expanded up to one meson ﬁeld,\\nAµ = i\\n2\\n\\x00ξ∂µξ† −ξ†∂µξ\\n\\x01 one Φ\\n−→\\n∂µΦ\\n√\\n2fπ\\n,\\nξ = exp\\n\\x12 iΦ\\n√\\n2fπ\\n\\x13\\n,\\n(8)\\nΦ, B, B are the standard SU(3) matrices of the meson and baryon ﬁelds, and fπ = 93 MeV. The Lagrangian in Eq.\\n(7) allows one to relate the πΛΣ∗coupling to πN∆. For the vertex one ﬁnds −itB∗→BΦ = a S · q where\\naΣ∗0→π0Λ = 0.82\\n√\\n2\\nfπN∆\\nmπ\\n,\\naΣ∗+→π+Λ = −0.82\\n√\\n2\\nfπN∆\\nmπ\\n,\\naΣ∗+→¯\\nK0p = 2\\n√\\n6\\n5\\nD + F\\n2fπ\\n,\\naΣ∗+→π+Σ0 = −0.78\\n√\\n6\\nfπN∆\\nmπ\\n,\\naΣ∗+→π0Σ+ = 0.78\\n√\\n6\\nfπN∆\\nmπ\\n,\\na∆++→π+p = fπN∆\\nmπ\\n(9)\\nwith q the momentum of the outgoing meson in the Σ∗or ∆rest frame and S the spin transition operator from 3/2\\nto 1/2 normalized as\\n⟨M|S†\\nµ|m⟩= C (1/2 1 3/2; m µ M) .\\n(10)\\nFor the fπN∆we take fπN∆= 2.13 to give the experimental ∆width. Note that SU(3) symmetry, implicit in Eq.\\n(9), is not exact. In order to obtain the experimental Σ∗→πΛ, Σ∗→πΣ widths one can ﬁt the coupling C from Eq.\\n(7) to the branching ratios from the PDG [17]. This leads to a correction which appears as a numerical factor in Eq.\\n(9). For the Σ∗→¯KN decay which is physically closed we use a SU(6) quark model prediction [39].\\nThe couplings from [21] of the ∆∗(1700) to its s-wave decay channels are given for I = 3/2 and counting the isospin\\ndecomposition of K0Σ∗0, K+Σ∗+, η∆, we ﬁnd the couplings\\ngK0Σ∗0∆∗0 =\\nr\\n2\\n3 gKΣ∗∆∗,\\ngK+Σ∗+∆∗++ = gKΣ∗∆∗,\\ngη∆++∆∗++ = gη∆∆∗.\\n(11)\\nAltogether, our amplitudes for the diagrams of Fig. 2 become\\n−it = a S · q G\\n1\\n√s∆∗−M∆∗+ iΓ∆∗(s∆∗)\\n2\\ngj g(d)\\ni\\nBW(kR)\\nBW(konR)\\n× C (1/2 2 3/2; m, M −m) Y2, m−M(ˆk)(−1)M−m√\\n4π\\n(12)\\nwith kon the pion momentum in the πN decay of the ∆∗(1700) at rest. Depending on the process, G = 1/(√sB∗−\\nMB∗+ i/2 ΓB∗(√sB∗)) is the Σ∗(1385) or ∆(1232) propagator; a in Eq. (12) is given by Eq. (9) and gj, g(d)\\ni\\nby Eqs.\\n(11) and (4), respectively.\\nFor the momentum-dependent width of the Σ∗(1385), we have taken into account the p-wave decays into πΛ and\\nπΣ with their respective branching ratios of 88% and 12%. For the width of the ∆∗(1700) we have included the\\ndynamics of the decay into ∆∗→Nρ(Nππ) and ∆∗→∆π(Nππ) in the same way as in Ref. [25]. We introduce a\\nnovelty with respect to Ref. [25] where the ρN decay of the N ∗(1520), ∆∗(1700) was considered in s-wave. In the case\\nof the ∆∗(1700) the d-wave ρN decay is mentioned in the PDG [17] as existing but with an undetermined strength.\\nWe have adopted here to take a ρN strength in s-wave of 37%, and 5% for ρN in d-wave after a ﬁne tuning to the\\ndata. For energies close to the ∆∗(1700) only the total width matters and this is about 300 MeV in our case. For the\\n5\\nwidth of the ∆∗→ρN in d-wave we use the formula of Ref. [25] (the equation before Eq. (37)) by multiplying the\\nnumerator by BW(|q1 −q2|2) and one coupling is adjusted to get the 5% of the branching ratio used.\\nWe should mention here that having ρN decay in s-wave or d-wave produces large diﬀerences at energies around\\n√s = 2 GeV and beyond, but only moderate diﬀerences close to the ∆∗(1700) peak or 100-150 MeV above it.\\nFor the ∆∗(1700) width from decay into πN in d-wave, an additional Blatt-Weisskopﬀfactor is applied to be\\nconsistent with Eq. (5). With the partial width Γ0\\nπN = 0.15 × 300 MeV, the width is given by [37]\\nΓπN = Γ0\\nπN\\nk BW 2(kR) M∆∗\\nkon BW 2(konR) √s\\n(13)\\nand the total width is the sum of the partial widths of the decay modes. The same is done for the d-wave of the ρN\\ndecay.\\nSumming |t|2 from Eq. (12) over the ﬁnal states, the sum does not depend on the original proton polarization. We\\nare free to choose m = 1/2 in which case the amplitude of Eq. (12) becomes\\n−it =\\na\\n√\\n3 G\\n1\\n√s∆∗−M∆∗+ iΓ∆∗(s∆∗)\\n2\\ngj g(d)\\ni\\nBW(kπR)\\nBW(kon\\nπ R)\\n(\\n2qz\\n; m′ = +1/2\\n−(qx + iqy) ; m′ = −1/2.\\n(14)\\nIn the sum of |t|2 over the ﬁnal states of Λ with m′ = 1/2, −1/2 the part corresponding to the curly bracket in Eq.\\n(14) will become\\n4q2\\nz + q2\\nx + q2\\ny = 3q2\\nz + q2\\n(15)\\nwhich gives an angular distribution proportional to (3 cos2 θ + 1) in the angle of the outgoing meson from the Σ∗or\\n∆decay with respect to the initial π−direction for this initial proton polarization. When integrating over angles Eq.\\n(15) can be replaced by 2q2. For the ﬁrst reaction from Fig. 2 the (π0Λ) invariant mass distribution is then given by\\ndσ\\ndMI(π0Λ) =\\nMpMΛ\\nλ1/2(s, m2π, M 2p)\\nqπ0qK0\\n(2π)3√s\\nX X\\n|t|2\\n(16)\\nin terms of the ordinary K¨allen function λ1/2 and t from Eq.\\n(14), qK0 = λ1/2(s, M 2\\nI , m2\\nK0)/(2√s), q ≡qπ0 =\\nλ1/2(M 2\\nI , m2\\nπ0, M 2\\nΛ)/(2MI). Furthermore, the variables in Eq. (14) take the values √s∆∗= √s, √sΣ∗= MI and the\\ntotal cross section is given by integrating over MI in Eq. (16). The generalization to other channels is straightforward\\nby changing the masses and corresponding momenta.\\nIII.\\nTHE MODEL FOR THE γp →KπΛ, kπΣ, ηπp REACTIONS\\nThe photon coupling to the ∆∗(1700) resonance is taken from [25, 40]. The processes γp →K0π+Λ, K+π−Σ+,\\nK+π+Σ−, K0π0Σ+, ηπ0p are given by diagrams similar to those of Fig. 2 with the incoming pion replaced by the\\nphoton. In view of this it is very easy to modify the pion-induced amplitudes of the previous section to write the\\nphoton-induced ones. The contributions for the ﬁrst four reactions are given by\\nT BG\\nγp→MMB = b 2\\n5\\nD + F\\n2fπ\\ngKΣ∗∆∗GΣ∗(√sΣ∗) G∆∗(√s∆∗)⃗S · pπ\\n×\\n\"\\n−ig′\\n1\\n⃗S† · k\\n2M (⃗σ × k) · ⃗ǫ −⃗S† · ⃗ǫ\\n\\x12\\ng′\\n1(k0 + k2\\n2M ) + g′\\n2\\n√s k0\\n\\x13#\\n(17)\\nwith\\nbγp→K0π+Λ = 1.04\\n√\\n3,\\nbγp→K+π−Σ+ = −\\n√\\n2,\\nbγp→K+π+Σ−=\\n√\\n2,\\nbγp→K0π0Σ+ = 1\\n(18)\\nwhich follows from Eq. (49) from Ref. [25] by applying the corresponding changes in isospin factors and corrections\\ndue to SU(3) breaking in the Σ∗(1385) decay similar as in Eq. (9). Cross sections and invariant masses for the photon\\nreactions are given by Eq. (16) with the corresponding changes in masses (e.g., mπ →0 in λ1/2). For the γp →ηπ0p\\nreaction we replace the factor before the brackets in Eq. (17) by\\n−\\nr\\n2\\n3 gη∆∆∗fπN∆\\nmπ\\nG∆∗(√s∆∗) G∆(√s∆).\\n(19)\\n6\\n1400\\n1500\\n1600\\n1700\\nplab [MeV]\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\nσ [µb]\\n1850\\n1900\\n1950\\n2000\\ns\\n1/2 [MeV]\\nπ\\n _ p --> K\\n0 π\\n0Λ\\n1300\\n1400\\n1500\\n1600\\n1700\\n1800\\n1900\\np lab [MeV]\\n0\\n100\\n200\\n300\\n400\\n500\\nσ [µb]\\n1800\\n1850\\n1900\\n1950\\n2000\\n2050\\n2100\\ns\\n1/2 [MeV]\\nπ\\n+ p --> K\\n+ π\\n+Λ\\n1600\\n1700\\n1800\\n1900\\np lab [MeV]\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nσ [µb]\\n1950\\n2000\\n2050\\n2100\\ns\\n1/2 [MeV]\\nπ\\n+ p --> K\\n+ K\\n0 p\\n1400\\n1500\\n1600\\n1700\\n1800\\n1900\\np lab [MeV]\\n0\\n10\\n20\\n30\\n40\\nσ [µb]\\n1850\\n1900\\n1950\\n2000\\n2050\\n2100\\ns\\n1/2 [MeV]\\nπ\\n+ p --> K\\n+ π\\n+Σ\\n0\\n1400\\n1500\\n1600\\n1700\\n1800\\n1900\\np lab [MeV]\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nσ [µb]\\n1850\\n1900\\n1950\\n2000\\n2050\\n2100\\ns\\n1/2 [MeV]\\nπ\\n+ p --> K\\n+ π\\n0Σ\\n+\\n1200\\n1300\\n1400\\n1500\\n1600\\n1700\\n1800\\n1900\\np lab [MeV]\\n0\\n200\\n400\\n600\\n800\\nσ [µb]\\n1800\\n1850\\n1900\\n1950\\n2000\\n2050\\n2100\\ns\\n1/2 [MeV]\\nπ\\n+ p --> η π\\n+p\\n1280\\n0\\n4.5\\n10\\nFIG. 3: Total cross sections for the pion-induced reactions. Data are from [27] (triangles up), [28] (triangle down), [18] (cross),\\n[29] (dots), [30] (diamonds). For the latter data, it is indicated that these are upper limits below p = 1.67 GeV as in Ref. [30].\\nThe γp →ηπ0 and γp →K0π0Σ+ reactions have been derived in Ref. [25]. Besides the tree level amplitudes there\\nare one-loop transitions between ∆∗(1700) and another dynamically generated resonance, the N ∗(1535). The latter\\nterms are not important at the high energies in which we are currently interested because the N ∗(1535) is oﬀ-shell.\\nIV.\\nRESULTS AND DISCUSSION\\nIn Figs. 3 to 5 we show the results for the pion- and photon-induced reactions. In Fig. 3 we show the cross sections\\nfor the π−p →K0π0Λ, π+p →K+π+Λ, π+p →K+ ¯K0p, π+p →K+π+Σ0, π+p →K+π0Σ+, and π+p →ηπ+p\\nreactions. The theoretical results are plotted in terms of a band. This band corresponds to taking the πN∆∗coupling\\n7\\nTheory:\\ndσ\\ndMI(π+Λ) [µb GeV−1]\\nExperiment: Arbitrary units\\n1300\\n1350\\n1400\\n1450\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1.2\\n1.4\\n1.6\\n1.7\\n1.8\\n1.9\\n2\\n0\\n1\\n2\\n3\\n4\\n5\\n1.7\\n1.8\\n1.9\\n2\\n2.1\\n0\\n2\\n4\\n6\\nMI(π0Λ) [MeV]\\nM 2\\nI (π+Λ) [GeV2]\\nM 2\\nI (π+Λ) [GeV2]\\nπ−p →K0π0Λ\\nπ+p →K+π+Λ\\nπ+p →K+π+Λ\\nplab = 1.508 GeV\\nplab = 1.42 GeV\\nplab = 1.55 GeV\\n(a)\\n(b)\\n(c)\\nFIG. 4: Invariant mass spectra for the π−p →K0π0Λ and π+p →K+π+Λ reactions. Experimental distributions (arbitrary\\nunits) are from [28] and [29], respectively.\\nwith its uncertainties (from the experimental branching ratio ) quoted in Eq. (3).\\nMuch of the data in Fig. 3 are for energies √s > 1950 MeV, which, even taking into account the 300 MeV width of\\nthe ∆∗(1700), are relatively far away from the ∆∗(1700) peak. In a situation like this it is logical to assume that other\\nchannels not related to the ∆∗(1700) could also play a role. Indeed, other partial waves in πN scattering are equally\\nimportant in this energy region [41, 42]. However, there are reasons to assume that they do not couple strongly to\\nthe KπΛ, KπΣ in the ﬁnal state at the lower energies, as we shall discuss at the end of this section. In any case, even\\nat the higher energies, the order of magnitude predicted for the cross section is correct.\\nFor the π−p →K0π0Λ reaction we have data around √s = 1930 MeV and the theory agrees with those data.\\nFurthermore, as we can see in [28], the π0Λ mass spectrum is totally dominated by the Σ∗(1385) and, as shown in Fig.\\n4 (a), the theoretical predictions agree with these data. It is also very instructive to see that the angular distribution\\nof the Σ∗is practically ﬂat [28, 43], as our model predicts, given the s-wave coupling of the ∆∗(1700) to KΣ∗.\\nNext we discuss the cross section for the π+p →K+π+Λ reaction. The range of energies extends now from about\\n√s = 1800 MeV on. We can see that the order of magnitude of the cross sections from the theoretical band is\\ncorrect, although the theoretical prediction is more than a factor of two bigger than data at around √s = 1900 MeV.\\nYet, this apparently large diﬀerence should be viewed in perspective, which is provided by the cross section of the\\nπ+p →K+π+Σ0 and π+p →K+π0Σ+ reactions. Indeed, given the larger coupling of the Σ∗(1385) resonance to πΛ,\\nwith a branching ratio to this latter channel about one order of magnitude larger than for πΣ, the mechanism that we\\nhave should provide a cross section for the π+p →K+π+Λ reaction about one order of magnitude bigger than for the\\nπ+p →K+π+Σ0 or π+p →K+π0Σ+ reactions. This is indeed the case, both in the theory and in the experiment.\\nWe can see that in the region of energies below √s < 1900 MeV the agreement of the theory with the data is ﬁne at\\nthe qualitative level for these two latter reactions. We can also see that the ﬁrst data point for the π+p →K+π+Λ\\nreaction is in agreement with the theoretical prediction (see the insert in Fig. 3)\\nThe invariant mass spectra for diﬀerent energies in the π+p →K+π+Λ reaction are shown in Fig. 4 (b), (c). These\\ncurves can be directly compared to the data of Ref. [29]; the Σ∗(1385) dominance in both theory and experiment is\\napparent. Note that, as mentioned in [28], the excess of strength at the lower shoulder is partly a result of the ﬁnite\\nexperimental resolution [28, 29].\\nIn Fig. 3 we also plot the cross section for the π+p →K+ ¯K0p reaction. We can see that the predicted cross sections\\nare quite low compared with experiment. This should be expected since our mechanism is doubly suppressed there,\\nﬁrst from having the ∆∗(1700) oﬀshell, and second from also having oﬀshell the Σ∗(1385) decaying into ¯KN. It is\\nthus not surprising that our mechanism produces these small cross sections. The Σ∗(1385) is, however, not oﬀshell\\nfor the πΣ and πΛ in the ﬁnal state, and even at the low energies of the ﬁgure the mass distribution for these two\\nparticles is dominated by the Σ∗(1385) in the theory, and this is also the case in the experiment as mentioned above.\\nFinally, we also show in Fig. 3 the cross section for the π+p →ηπ+p reaction. Here the mechanism is also ∆∗(1700)\\nproduction but it decays into η∆(1232) followed by ∆(1232) →πN. The agreement of the theory with the data is\\nfair for low energies, even more when we read the caution statement in the experimental paper [30] warning that the\\ndata are overestimated below plab = 1670 MeV. Once again it is worth noting that below plab = 1670 MeV the cross\\nsections are a factor ﬁfty larger than for π+p →K+π0Σ+ or π+p →K+π+Σ0. The theory is producing these large\\norder of magnitude changes in the cross sections correctly.\\n8\\n0\\n0.5\\n1\\n1.5\\n2\\n1800\\n1900\\n2000\\n2100\\ns\\n1/2 [MeV]\\n0\\n0.2\\n0.4\\n0.6\\nσ [µ b]\\n1250\\n1500\\n1750\\n2000\\nEγ [MeV]\\n0\\n0.2\\n0.4\\n0.6\\nγ p --> K\\n0 π\\n+ Λ\\nγ p --> K\\n+ π\\n - Σ\\n+\\nγ p --> K\\n+ π\\n+ Σ\\n -\\n1200\\n1400\\n1600\\nEγ [MeV]\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nσ [µ b]\\n1700\\n1800\\n1900\\n2000\\ns\\n1/2 [MeV]\\nγ p --> π\\n0 η p\\n1400\\n1500\\n1600\\n1700\\nEγ [MeV]\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\nσ [µ b]\\n1900\\n2000\\ns\\n1/2 [MeV]\\nγ p --> π\\n0 K\\n0 Σ\\n+ \\nFIG. 5: Photoproduction of strange and η particles. Data are from [32] (dots), [33] (crosses), and [34] (triangles up). The latter\\ndata are the sum of KΛπ and KΣπ ﬁnal states. The two lower plots show the update of the predictions from Ref. [25] (gray\\nbands) compared to the results from [25] (solid lines).\\nNext we discuss the photonuclear cross sections. In Fig. 5 we can see the cross sections for the γp →K0π+Λ,\\nγp →K+π−Σ+, γp →K+π+Σ−, γp →K0π0Σ+ and γp →ηπ0p reactions. For the ﬁrst three reactions we have some\\ndata from [32, 33, 34]. Only a few data points appear in the region below √s = 1900 MeV and, furthermore, the data\\nhave large uncertainties, both in the magnitude of the cross section and in the value of √s. Nevertheless, the data\\nare valuable in this global analysis that we are doing. In the ﬁrst place we can see that within errors, the agreement\\nof theory and experiment is fair. Yet, more signiﬁcant is the ratio of more than one order of magnitude, both in the\\ntheory and experiment for the cross sections of the γp →ηπ0p and the γp →K+π+Σ−or γp →K+π−Σ+ reactions.\\nThis follows the same trend as the π+p →ηπ+p and π+p →K+π+Σ0 or π+p →K+π0Σ+ reactions discussed above,\\nsupporting also the dominance of the same mechanisms in the reactions. It is also worth noting that the dominance\\nof the Σ∗(1385) production for the γp →K0π+Λ reaction is also mentioned in Ref. [33].\\n9\\nIn the ﬁgure we also plot the cross sections for the γp →K0π0Σ+ and γp →ηπ0p reactions measured at ELSA and\\nwhich are presently being analyzed [26]. These latter two reactions were studied in [25] with far more detail including\\nmany more mechanisms.\\nYet, it was this detailed study that showed the dominance of the reaction mechanism\\nconsidered in this paper. We also found there that around √s = 2000 MeV, the extra terms could modify the cross\\nsection by about 30 percent or more. For these two reactions we have taken the full model of [25]. We show now a\\nband of values given by the uncertainties from the experimental helicity amplitudes of the γp∆∗transition [17] for\\nthe cross section. Furthermore, we show that both cross sections have been reduced by about 30 percent with respect\\nto those in [25] as a consequence of the consideration of a more realistic ∆(1700) width, larger now and closer to the\\nexperimental 300 MeV, and in addition we have also taken a small fraction of the ρN decay width in d-wave. These\\nlatter two cross sections are also in qualitative agreement with preliminary results for the cross sections as shown in\\n[26].\\nThe consideration of the diﬀerent reactions, with the cross sections spanning nearly two orders of magnitude,\\nand the global qualitative agreement found for all the diﬀerent reactions, gives support to the reaction mechanism\\nsuggested here in which a ∆∗(1700) resonance is excited which decays later on into KΣ∗or η∆. The predictions\\nof the cross sections are tied to the couplings of the ∆∗(1700) resonance to the KΣ∗or η∆channels provided by\\nthe hypothesis that the ∆∗(1700) is a dynamically generated resonance. We showed in the Introduction that the\\ncouplings to these channels were substantially diﬀerent than those provided by a simple SU(3) symmetry and there\\nis hence substantial dynamical information from the underlying chiral dynamics and coupled-channels unitarity. In\\nthis respect the global analysis of these reactions oﬀers support to the basic idea about the nature of the ∆∗(1700)\\nas a dynamically generated resonance. No doubt a more detailed theoretical analysis should consider extra terms, as\\ndone for instance in [25] and also extra mechanisms beyond 200 or 300 MeV above the ∆∗(1700) region. However,\\nthe global qualitative agreement, considering the large span of the diﬀerent cross sections, and that a simple SU(3)\\nsymmetrical consideration would produce cross sections about a factor 30 diﬀerent, indicate that the agreement found\\nhere is not a trivial thing.\\nAt this point we would like to make some comments about other contributions to some of the reactions discussed\\nand related ones that we mention below. First we mention that in the case of the γp →K+π−Σ+ and γp →K+π+Σ−\\nreactions there can be a contamination from the K+Λ(1405) production. This latter reaction was studied in [44] and\\nis presently investigated experimentally at Spring8/Osaka [45]. We have recalculated the cross section for γp →\\nK+Λ(1405) with the model of [44] and ﬁnd that the cross sections are of the same order of magnitude, or smaller in\\nthe case of the γp →K+π+Σ−reaction, than those shown in Fig. 5. Given the qualitative use made of the cross\\nsections for these two reactions in the discussion above, and the large errors in the data, the conclusions drawn are not\\naﬀected. Note also that for the γp →K0π+Λ reaction the Λ(1405) production is not allowed and there the dominance\\nof the Σ∗(1385) production has stronger grounds. Note also that this latter reaction has a larger cross section than\\nthe other two from our KΣ∗(1385) production mechanisms which should make potential extra contributions relatively\\nsmaller.\\nAnother possible mechanism can come from having the two mesons in a resonant state, the K∗(892). Yet, the\\nreactions γp(πp) →∆∗(1700) →K∗Λ are not allowed by isospin conservation, and precisely the reactions with Λ in\\nthe ﬁnal state are those with the largest cross sections. The sequence ∆∗(1700) →K∗Σ is possible and this can be a\\nsource of background for the reactions with lower cross sections. Of course we could also have γp(πp) →K∗Λ without\\npassing through a I = 3/2 channel and then this ﬁnal state would also be allowed. However, note that the thresholds\\nfor the K∗Λ and K∗Σ production are 2007 MeV and 2089 MeV, respectively. Hence, for energies lower than these\\nby 100 or 200 MeV this contribution should be highly suppressed. This is indeed an experimental fact as noted in\\n[28]. Yet, in some reactions the K∗Λ contamination is more visible than in others, for instance in the π−p →K+π−Λ\\nreaction it is more apparent than for π−p →K0π0Λ [27]. This could explain why our model is short by about 30% in\\nthe π−p →K+π−Λ reaction cross section at 1930 MeV [27] (not shown but also calculated) while it is good for the\\nπ−p →K0π0Λ reaction.\\nSimilar comments can be made about the π−p →K+π0Σ−and π−p →K+π−Σ0 reactions which in our model\\ngo via K+Σ∗−and, hence, can have I = 1/2, 3/2 while the π+p →K+π+Σ0 and π+p →K+π0Σ+ reactions shown\\nin Fig. 3 go through K+Σ∗+ and, hence, have only I = 3/2 and larger chances to couple to the ∆∗(1700) . The\\nClebsch-Gordan coeﬃcients of π+p →∆∗++ or π−p →∆∗0 also favor the π+p reactions and indeed we ﬁnd cross\\nsections for the π−p →K+π0Σ−and π−p →K+π−Σ0 reactions substantially smaller (about one order of magnitude).\\nWith such smaller cross sections it should be expected that background terms become more relevant and, hence, these\\nreactions are not considered for our tests.\\nFinally, let us mention the possible contribution in the entrance channels of other resonances, apart from the\\n∆∗(1700). The N ∗(1700)D13 can be a candidate which would possibly aﬀect the π−p reactions but not the π+p\\nreactions. In any case, the smaller width of the N ∗(1700)D13 (50-150 MeV) does not give much chance for contributions\\nat the threshold of the reactions. Other possible N ∗or ∆∗resonances in the regions of the energies below 2000\\nMeV, considering their spin and parity, can be ruled out on the basis of the s-wave KΣ∗dominance experimentally\\n10\\nestablished in [28, 43].\\nThe only possible exception is the one-star ∆(1940)D33 resonance for which no KΣ∗or\\nη∆(1232) decay channels are reported in spite of being allowed by phase space and, hence, it is not considered here.\\nThe experimental s-wave KΣ∗dominance at low energies served as support to our theory, but we should note that\\nat √s = 2020 MeV in the π−p →K0π0Λ reaction, the s-wave KΣ∗dominance from the √s = 1930 data points [28, 43]\\ndoes no longer hold; in fact, the production angular distribution shows a forward peak [18] which is well described by\\nt-channel K∗exchange in the framework of the Stodolsky-Sakurai model [18]. Similarly, the forward peak for the high\\nenergies is observed in the π+p →K+π+Λ reaction [29]. This does not rule out our production mechanisms since the\\nWeinberg-Tomozawa term used to generate the ∆∗(1700) in the Bethe-Salpeter equation [21] eﬀectively accounts for\\na vector meson exchange in the t-channel (in the limit of small momentum transfer). Improvements could be done\\nin the theory to account explicitly for the ﬁnite momentum transfer dependence, as done in [8] but, since this only\\naﬀects the larger energies, we do not consider it here.\\nV.\\nCONCLUSIONS\\nWe have looked at several pion-induced and photon-induced reactions at energies above and close to the ∆∗(1700)\\nwith KπΛ, KπΣ and ηπN in the ﬁnal state. We have made a theoretical model assuming that a ∆∗(1700) is excited\\nand then decays via the KΣ∗(1385) or η∆(1232) depending on the ﬁnal state. We ﬁnd that in spite of exploiting\\nthe tail of the resonance, around half of the width or one width above the nominal energy of the ∆∗(1700), the cross\\nsections obtained are sizable. The reason is the large couplings of the ∆∗(1700) to the KΣ∗(1385) or η∆(1232) which\\nare provided by the theory in which the ∆∗(1700) is a dynamically generated resonance. We showed that the couplings\\nsquared to these channels were about 20-30 times bigger than estimated by simple SU(3) symmetry arguments. We\\ncould also see that the presence of the Σ∗(1385) was clear in the experimental data with KπΛ, KπΣ ﬁnal states, with\\nlittle room for background at low energies, indicating a clear dominance of the πΛ, πΣ ﬁnal states in the Σ∗(1385)\\nchannel. Despite the admitted room for improvements in the theory, the qualitative global agreement of the diﬀerent\\ncross section with the data gives a strong support to the mechanisms proposed here and the strong couplings of the\\n∆∗(1700) to the KΣ∗(1385) or η∆(1232) channels. The agreement found is more signiﬁcant when one realizes the\\nlarge diﬀerence in magnitude of the diﬀerent cross sections and the clear correlation of the theoretical predictions\\nwith the data.\\nThe results obtained are relevant because they rely upon the ∆∗(1700) couplings to KΣ∗and η∆(1232) for which\\nthere is no experimental information, but which are provided by the theory in which the ∆∗(1700) is dynamically\\ngenerated.\\nThe next question arises on what could be done in the future to make a more quantitative calculation. A number\\nof factors is needed to go forward in this direction:\\n1. The experimental total width of the ∆∗(1700) and the branching ratio to πN and ρN should be improved. The\\nseparation of the ρN channel in s- and d- waves needs also to be performed if accurate predictions are to be\\ndone for 200-300 MeV above the ∆∗(1700) resonance region.\\n2. Experiments at lower energies, closer to the ∆∗(1700) energy, for the photon- and pion-induced reactions dis-\\ncussed, would be most welcome. There the ∆∗(1700) excitation mechanism would be more dominant and one\\nwould reduce uncertainties from other possible background terms.\\n3. Some improvements on the generation of the ∆∗(1700), including extra channels to the π∆, KΣ∗, and η∆used\\nin Refs. [20, 21], like πN in d-wave and ρN in s- and d-waves, would be most welcome, thus helping ﬁne tune\\nthe present coupling of ∆∗(1700) to KΣ∗and η∆provided by [21].\\nAwaiting progress in these directions, at the present time we could claim, that within admitted theoretical and\\nexperimental uncertainties, the present data for the large sample of pion and photon-induced reactions oﬀer support\\nfor the large coupling of the ∆∗(1700) resonance to KΣ∗and η∆predicted by the chiral unitary approach for which\\nthere was no previous experimental information.\\nAcknowledgments\\nThis work is partly supported by DGICYT contract number BFM2003-00856, and the E.U. EURIDICE network\\ncontract no. HPRN-CT-2002-00311. This research is part of the EU Integrated Infrastructure Initiative Hadron\\n11\\nPhysics Project under contract number RII3-CT-2004-506078.\\n[1] R.H. Dalitz and S.F. Tuan, Ann. Phys. (N.Y.) 10, 307 (1960)\\n[2] B. K. Jennings, Phys. Lett. B 176, 229 (1986).\\n[3] N. Kaiser, P. B. Siegel and W. Weise, Phys. Lett. B 362 (1995) 23\\n[4] N. Kaiser, T. Waas and W. Weise, Nucl. Phys. A 612 (1997) 297\\n[5] E. Oset and A. Ramos, Nucl. Phys. A 635 (1998) 99.\\n[6] J. C. Nacher, A. Parreno, E. Oset, A. Ramos, A. Hosaka and M. Oka, Nucl. Phys. A 678 (2000) 187\\n[7] J. A. Oller and U.-G. Meißner, Phys. Lett. B 500 (2001) 263.\\n[8] T. Inoue, E. Oset and M. J. Vicente Vacas, Phys. Rev. C 65 (2002) 035204\\n[9] E. Oset, A. Ramos and C. Bennhold, Phys. Lett. B 527 (2002) 99 [Erratum-ibid. B 530 (2002) 260]\\n[10] C. Garcia-Recio, J. Nieves, E. Ruiz Arriola and M. J. Vicente Vacas, Phys. Rev. D 67 (2003) 076009\\n[11] D. Jido, J. A. Oller, E. Oset, A. Ramos and U. G. Meissner, Nucl. Phys. A 725 (2003) 181\\n[12] C. Garcia-Recio, M. F. M. Lutz and J. Nieves, Phys. Lett. B 582 (2004) 49\\n[13] B. Borasoy, R. Nissler and W. Weise, Eur. Phys. J. A 25, 79 (2005)\\n[14] J. A. Oller, J. Prades and M. Verbeni, Phys. Rev. Lett. 95, 172502 (2005)\\n[15] J. A. Oller, arXiv:hep-ph/0603134.\\n[16] S. Prakhov et al. [Crystall Ball Collaboration], Phys. Rev. C 70, 034605 (2004).\\n[17] S. Eidelman et al. [Particle Data Group], Phys. Lett. B 592, 1 (2004).\\n[18] D. W. Thomas, A. Engler, H. E. Fisk and R. W. Kraemer, Nucl. Phys. B 56, 15 (1973).\\n[19] V. K. Magas, E. Oset and A. Ramos, Phys. Rev. Lett. 95, 052301 (2005) [arXiv:hep-ph/0503043].\\n[20] E. E. Kolomeitsev and M. F. M. Lutz, Phys. Lett. B 585 (2004) 243\\n[21] S. Sarkar, E. Oset and M. J. Vicente Vacas, Nucl. Phys. A 750 (2005) 294\\n[22] S. Sarkar, E. Oset and M. J. Vicente Vacas, Phys. Rev. C 72, 015206 (2005)\\n[23] L. Roca, S. Sarkar, V. K. Magas and E. Oset, Phys. Rev. C 73, 045208 (2006)\\n[24] M. D¨oring, E. Oset and S. Sarkar, arXiv:nucl-th/0601027.\\n[25] M. D¨oring, E. Oset and D. Strottman, Phys. Rev. C 73, 045209 (2006), arXiv:nucl-th/0510015.\\n[26] M. Nanova at the ”International Workshop On The Physics Of Excited Baryons (NSTAR 05)”, 10-15 Oct 2005, Tallahassee,\\nFlorida\\n[27] O. I. Dahl, L. M. Hardy, R. I. Hess et.al., Phys. Rev. 163, 1337 (1967).\\n[28] L. J. Curtis, C. T. Coﬃn, D. I. Meyer, and K. M. Terwilliger, Phys. Rev. 132, 1771 (1963).\\n[29] P. Hanson, G. E. Kalmus and J. Louie, Phys. Rev. D 4, 1296 (1971).\\n[30] D. Grether, G. Gidal and G. Borreani, Phys. Rev. D 7, 3200 (1973).\\n[31] T. Hyodo, A. Hosaka, E. Oset, A. Ramos and M. J. Vicente Vacas, Phys. Rev. C 68, 065203 (2003)\\n[32] R. Erbe et al. [Aachen-Berlin-Bonn-Hamburg-Heidelberg-Muenchen Collaboration], Phys. Rev. 188, 2060 (1969).\\n[33] R. Erbe et al. [Aachen-Berlin-Bonn-Hamburg-Heidelberg-Muenchen Collaboration], Nuovo Cimento 49A, 504 (1967).\\n[34] Cambridge Bubble Chamber Group, Phys. Rev. 156, 1426 (1966)\\n[35] J.M. Blatt and V.F. Weisskopﬀ, Theoretical Nuclear Physics (Wiley, New York, 1952)\\n[36] D. M. Manley, R. A. Arndt, Y. Goradia and V. L. Teplitz, Phys. Rev. D 30, 904 (1984).\\n[37] D. M. Manley and E. M. Saleski, Phys. Rev. D 45, 4002 (1992).\\n[38] M. N. Butler, M. J. Savage and R. P. Springer, Nucl. Phys. B 399, 69 (1993)\\n[39] E. Oset and A. Ramos, Nucl. Phys. A 679, 616 (2001)\\n[40] J. C. Nacher, E. Oset, M. J. Vicente and L. Roca, Nucl. Phys. A 695, 295 (2001)\\n[41] R. A. Arndt, W. J. Briscoe, I. I. Strakovsky, R. L. Workman and M. M. Pavan, Phys. Rev. C 69 (2004) 035213\\n[42] T. P. Vrana, S. A. Dytman and T. S. H. Lee, Phys. Rept. 328 (2000) 181\\n[43] O. Goussu, M. Sen´e, B. Ghidini et.al., Nuovo Cimento 42A, 606 (1966)\\n[44] J. C. Nacher, E. Oset, H. Toki and A. Ramos, Phys. Lett. B 455 (1999) 55\\n[45] J. K. Ahn [LEPS Collaboration], Nucl. Phys. A 721 (2003) 715.\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=ArxivLoader(query='1700',load_max_docs=2).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nSince its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet\\'s automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\n\\n\\n=== Academic artificial intelligence ===\\nThe academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince its inception, the field of machine learning used both discriminative models and generative models, to mode'),\n",
       " Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography refers to digitally created explicit content produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography refers to digitally created explicit content produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated NCII (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Francisco filed a landmark lawsuit to shut down \"undress\" apps that allow users to generate non-consensual AI nude images, citing violations of state laws. The case aligns with California\\'s recent legislation—SB 926, SB 942, and SB 981—championed by Senators Aisha Wahab and Josh Becker an')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WikipediaLoader(query='generativeAI',load_max_docs=2).load()\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
